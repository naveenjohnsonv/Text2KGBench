{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7204faf9-d1b1-475f-890b-519ac0051d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import http.client\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from SPARQLWrapper.SPARQLExceptions import QueryBadFormed, EndPointInternalError, EndPointNotFound, Unauthorized, URITooLong, SPARQLWrapperException\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5b6466-2d46-4f27-b0ce-e14530a8cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query_cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1d24a2-dc53-444b-8aeb-887afa3150f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_date_string(date_string):\n",
    "    pattern = r\"^(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})Z$\"\n",
    "    match = re.match(pattern, date_string)\n",
    "    if match:\n",
    "        year, month, day, hour, minute, second = match.groups()\n",
    "        date = datetime(int(year), int(month), int(day))\n",
    "        month_name = date.strftime(\"%B\")\n",
    "        new_date_string = f\"{day} {month_name} {year}\"\n",
    "        return new_date_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_splits(triples, splits = [0.4, 0.3, 0.3]):\n",
    "    triples = np.array(triples)\n",
    "    indices = np.random.permutation(triples.shape[0])\n",
    "    train_count = int(triples.shape[0] * splits[0])\n",
    "    val_count = int(triples.shape[0] * splits[1])\n",
    "    test_count = triples.shape[0] - train_count - val_count\n",
    "    train_triples = triples[indices[:train_count]]\n",
    "    val_triples = triples[indices[train_count:train_count+val_count]]\n",
    "    test_triples = triples[indices[train_count+val_count:]]\n",
    "    return train_triples.tolist(), val_triples.tolist(), test_triples.tolist()\n",
    "\n",
    "def save_triples(onto_id, train_all, val_all, test_all):\n",
    "    # Define base paths for different directories\n",
    "    base_paths = {\n",
    "        'train': \"../../data/wikidata_tekgen/train\",\n",
    "        'validation': \"../../data/wikidata_tekgen/validation\",\n",
    "        'ground_truth': \"../../data/wikidata_tekgen/ground_truth\",\n",
    "        'test': \"../../data/wikidata_tekgen/test\"\n",
    "    }\n",
    "\n",
    "    # Ensure all required directories exist\n",
    "    for path in base_paths.values():\n",
    "        ensure_directory_exists(path)\n",
    "\n",
    "    # Save train data\n",
    "    with open(f\"{base_paths['train']}/{onto_id}_train.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(train_all):\n",
    "            data = {\"id\": f\"{onto_id}_train_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2],\n",
    "                    \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Save validation data\n",
    "    with open(f\"{base_paths['validation']}/{onto_id}_validation.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(val_all):\n",
    "            data = {\"id\": f\"{onto_id}_val_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2],\n",
    "                    \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Group test triples by sentences and remove duplicate triples\n",
    "    sentence_to_triples = defaultdict(set)\n",
    "    for tr in test_all:\n",
    "        sent = tr[6]\n",
    "        triple_tuple = (tr[0], tr[1], tr[2])  # Using labels: sub_label, rel_label, obj_label\n",
    "        sentence_to_triples[sent].add(triple_tuple)\n",
    "\n",
    "    # Save ground truth data\n",
    "    with open(f\"{base_paths['ground_truth']}/{onto_id}_ground_truth.jsonl\", \"w\") as out_file:\n",
    "        for idx, (sent, triples_set) in enumerate(sentence_to_triples.items()):\n",
    "            # Convert set of tuples back to list of dicts\n",
    "            triples = [{\"sub\": sub, \"rel\": rel, \"obj\": obj} for (sub, rel, obj) in triples_set]\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": sent, \"triples\": triples}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Save test data\n",
    "    with open(f\"{base_paths['test']}/{onto_id}_test.jsonl\", \"w\") as out_file:\n",
    "        for idx, (sent, _) in enumerate(sentence_to_triples.items()):\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": sent}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "def get_triples_with_sentences(relation_pid: str, relation_label: str, rel_domain: str, rel_range: str,\n",
    "                               limit: int = 200, max_retries: int = 10):\n",
    "    assert relation_pid, \"relation id can't be empty\"\n",
    "    assert rel_domain, \"domain can't be empty\"\n",
    "\n",
    "    current_limit = 10000  # Start with a high limit for SPARQL query\n",
    "    retries = 0\n",
    "    # Set the User-Agent according to Wikidata's policy\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Build the SPARQL query\n",
    "            sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "            query = \"PREFIX wdt: <http://www.wikidata.org/prop/direct/> \\n PREFIX wd: <http://www.wikidata.org/entity/> \\n\"\n",
    "            query += \"SELECT DISTINCT ?sub ?subEntity ?objEntity ?objLabel { \\n ?subEntity wdt:P31/wdt:P279* wd:\" + rel_domain + \" . \\n\"\n",
    "            query += '?subEntity rdfs:label ?sub . FILTER (lang(?sub) = \"en\") \\n '\n",
    "            query += '?subEntity wdt:' + relation_pid + ' ?objEntity . \\n'\n",
    "            if rel_range and rel_range != \"\":\n",
    "                query += '?objEntity wdt:P31*/wdt:P279* wd:' + rel_range + ' . \\n '\n",
    "            query += 'OPTIONAL { ?objEntity rdfs:label ?objLabel . FILTER (lang(?objLabel) = \"en\") } \\n } '\n",
    "            # Set the dynamic LIMIT\n",
    "            query += f\"LIMIT {current_limit}\"\n",
    "            if show_query:\n",
    "                print(query)\n",
    "\n",
    "            # Generate a hash of the query string to use as a cache filename\n",
    "            query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()\n",
    "            cache_directory = 'sparql_cache'\n",
    "            cache_filename = os.path.join(cache_directory, f'{query_hash}.json')\n",
    "\n",
    "            # Ensure the cache directory exists\n",
    "            os.makedirs(cache_directory, exist_ok=True)\n",
    "\n",
    "            # Check if the cache file exists\n",
    "            if os.path.exists(cache_filename):\n",
    "                # Load the cached results from the file\n",
    "                with open(cache_filename, 'r', encoding='utf-8') as cache_file:\n",
    "                    cached_data = json.load(cache_file)\n",
    "                    triples = cached_data['triples']\n",
    "                    print(\"Loaded results from cache.\")\n",
    "                # Store in the in-memory cache as well\n",
    "                sparql_query_cache[query] = triples\n",
    "            elif query in sparql_query_cache:\n",
    "                # Use cached results from in-memory cache\n",
    "                triples = sparql_query_cache[query]\n",
    "            else:\n",
    "                # Execute the query and get a set of triples\n",
    "                triples = list()\n",
    "                subject_counter, object_counter = Counter(), Counter()\n",
    "                secondary_triples = list()\n",
    "                sparql.setQuery(query)\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                sparql.setTimeout(300)  # Set timeout to 5 minutes\n",
    "                sparql.setMethod('POST')\n",
    "\n",
    "                # Set the User-Agent\n",
    "                sparql.agent = user_agent\n",
    "\n",
    "                # Attempt to execute the query\n",
    "                response = sparql.query()\n",
    "                results = response.convert()\n",
    "\n",
    "                print(f'  {len(results[\"results\"][\"bindings\"])} SPARQL results.')\n",
    "                for result in results[\"results\"][\"bindings\"]:\n",
    "                    t_subject = result['sub']['value']\n",
    "                    if 'objLabel' in result:\n",
    "                        t_object = result['objLabel']['value']\n",
    "                        t_object_id = result['objEntity']['value'].replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                    else:\n",
    "                        t_object = result['objEntity']['value']\n",
    "                        date_string = convert_date_string(t_object)\n",
    "                        if date_string:\n",
    "                            t_object = date_string\n",
    "                        t_object_id = None\n",
    "                    t_subject_id = result['subEntity']['value'].replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                    triple = [t_subject, relation_label, t_object, t_subject_id, relation_pid, t_object_id]\n",
    "                    # To get a diverse dataset, ignore subject/object if they occur more than 10% of the limit\n",
    "                    subject_counter[t_subject] += 1\n",
    "                    object_counter[t_object] += 1\n",
    "                    if subject_counter[t_subject] > (limit / 10) or object_counter[t_object] > (limit / 10):\n",
    "                        secondary_triples.append(triple)\n",
    "                        continue\n",
    "                    triples.append(triple)\n",
    "\n",
    "                # Append secondary triples\n",
    "                triples += secondary_triples\n",
    "                # Save to in-memory cache\n",
    "                sparql_query_cache[query] = triples\n",
    "                # Save results to cache file\n",
    "                with open(cache_filename, 'w', encoding='utf-8') as cache_file:\n",
    "                    json.dump({'triples': triples}, cache_file)\n",
    "                print(\"Saved results to cache.\")\n",
    "\n",
    "            print(f\"  collected {len(triples)} triples\")\n",
    "            if show_sample:\n",
    "                print(f\"  sample:\")\n",
    "                for tr in triples[:5]:\n",
    "                    print(f\"    {tr[:3]}\")\n",
    "\n",
    "            triples_with_sentences = list()\n",
    "            for tr in triples:\n",
    "                search_key = create_key(tr[0], tr[1], tr[2])\n",
    "                if search_key in sent_index:\n",
    "                    sentence = sent_index[search_key]\n",
    "                else:\n",
    "                    continue\n",
    "                tr.append(sentence)\n",
    "                triples_with_sentences.append(tr)\n",
    "\n",
    "                # Stop at the desired limit\n",
    "                if len(triples_with_sentences) >= limit:\n",
    "                    break\n",
    "\n",
    "            # If successful, return the collected triples with sentences\n",
    "            return triples_with_sentences\n",
    "\n",
    "        except SPARQLWrapperException as e:\n",
    "            retries += 1\n",
    "            code = None\n",
    "            reason = str(e)\n",
    "\n",
    "            # Try to parse the HTTP status code from the exception message\n",
    "            match = re.search(r'status code (\\d+)', reason, re.IGNORECASE)\n",
    "            if match:\n",
    "                code = int(match.group(1))\n",
    "\n",
    "            if code == 429:\n",
    "                # HTTP 429: Too Many Requests\n",
    "                retry_after = '60'  # Default wait time\n",
    "                wait_time = int(retry_after)\n",
    "                print(f\"HTTP 429 error encountered. Waiting for {wait_time} seconds before retrying ({retries}/{max_retries})...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif code == 500:\n",
    "                print(f\"HTTP 500 error encountered on attempt {retries}/{max_retries}. Reducing LIMIT and retrying...\")\n",
    "                current_limit = max(10, current_limit // 2)\n",
    "                #time.sleep(5)\n",
    "            else:\n",
    "                print(f\"HTTP error {code} encountered: {reason}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "                time.sleep(5)\n",
    "        except (http.client.IncompleteRead, json.JSONDecodeError) as e:\n",
    "            retries += 1\n",
    "            print(f\"An error occurred: {e}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "            # Reduce the LIMIT and retry\n",
    "            current_limit = max(10, current_limit // 2)\n",
    "            print(f\"Reducing LIMIT to {current_limit} and retrying...\")\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"An error occurred: {e}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "            #time.sleep(5)\n",
    "\n",
    "    print(\"Max retries reached. Skipping this relation.\")\n",
    "    return []\n",
    "\n",
    "def create_key(sub_label, rel_label, obj_label):\n",
    "    # remove spaces and make lower case\n",
    "    sub_label = re.sub(r\"\\s+\", '', sub_label).lower()\n",
    "    rel_label = re.sub(r\"\\s+\", '', rel_label).lower()\n",
    "    obj_label = re.sub(r\"\\s+\", '', obj_label).lower()\n",
    "    # concatanate them \n",
    "    tr_key = f\"{sub_label}{rel_label}{obj_label}\"\n",
    "    return tr_key\n",
    "\n",
    "def ensure_directory_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1477d7d4-c1f3-4123-b2a4-cd28ba55a03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TekGen corpus processing started!\n",
      "\ttriple-to-sent index with 11358950 triples loaded in 1.25 mins!\n"
     ]
    }
   ],
   "source": [
    "# Load the TekGen corpus\n",
    "sent_index = dict()\n",
    "start_time = time.time()\n",
    "print(\"TekGen corpus processing started!\")\n",
    "with open('../../tekgen.csv') as csv_in_file:\n",
    "    sent_reader = csv.reader(csv_in_file)\n",
    "    next(sent_reader)\n",
    "    for row in sent_reader:\n",
    "        tr_key = create_key(row[0], row[1], row[2])\n",
    "        sent = row[4]\n",
    "        sent_index[tr_key] = sent\n",
    "        elapsed_time = (time.time()-start_time)/60\n",
    "    print(f\"\\ttriple-to-sent index with {len(sent_index)} triples loaded in {elapsed_time:.2f} mins!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c801336-be11-4395-83eb-d889b334289e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology: Culture Ontology (ont_10_culture)\n",
      "\n",
      "processing \"ethnic group\" (P172) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Mohamad Supardi Md Noor', 'ethnic group', 'Malays']\n",
      "    ['Lucas Fernandez de Piedrahita', 'ethnic group', 'Quechua people']\n",
      "    ['Ruslan Mamat', 'ethnic group', 'Malays']\n",
      "    ['Ratu Sikumbang', 'ethnic group', 'Malays']\n",
      "    ['Eva Copa', 'ethnic group', 'Aymara']\n",
      "    34 triples with sentences in 0.10 seconds!\n",
      "\n",
      "processing \"religious order\" (P611) relation:\n",
      "  collected 0 triples\n",
      "  sample:\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"languages spoken, written or signed\" (P1412) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Chao Tien-lin', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Chen Ou-po', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Chiu Chi-wei', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Chang Chih-Chia', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Ang Ui-jin', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    200 triples with sentences in 0.07 seconds!\n",
      "\n",
      "processing \"inception\" (P571) relation:\n",
      "  collected 334 triples\n",
      "  sample:\n",
      "    ['Picture Book of the Eternal Pines', 'inception', '01 January 1767']\n",
      "    ['Il giornalino della Domenica', 'inception', '01 January 1906']\n",
      "    ['Eastern Brocade of One Hundred Poems by One Hundred Poets', 'inception', '01 January 1775']\n",
      "    ['Thirty days hath September', 'inception', '01 January 1500']\n",
      "    [\"New Year's Day at the Ogiya Seiro\", 'inception', '01 January 1804']\n",
      "    3 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"start time\" (P580) relation:\n",
      "  collected 618 triples\n",
      "  sample:\n",
      "    ['Calgary Folk Music Festival 2023', 'start time', '27 July 2023']\n",
      "    ['Calgary Folk Music Festival 2024', 'start time', '25 July 2024']\n",
      "    ['XVI Krajowy Festiwal Piosenki Polskiej w Opolu', 'start time', '19 June 1978']\n",
      "    ['Toronto International Film Festival', 'start time', '01 October 1976']\n",
      "    ['Hop Gala 1998', 'start time', '01 September 1998']\n",
      "    14 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"dedicated to\" (P825) relation:\n",
      "  collected 17 triples\n",
      "  sample:\n",
      "    ['Agustín Rodríguez Sahagún', 'dedicated to', 'Agustín Rodríguez Sahagún']\n",
      "    ['Pianta di Roma (Pérac-Lafréry)', 'dedicated to', 'Henry III of France']\n",
      "    ['Essex, actually surveyed, with the several Roads from London...', 'dedicated to', 'Arthur Capell, 1st Earl of Essex']\n",
      "    ['2551913', 'dedicated to', 'Jean Jaurès']\n",
      "    ['Hommage à Arago', 'dedicated to', 'François Arago']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"iconographic symbol\" (P4185) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "  collected 39 triples\n",
      "  sample:\n",
      "    ['Philomena', 'iconographic symbol', \"martyr's palm\"]\n",
      "    ['Paul the Apostle', 'iconographic symbol', 'elephant ivory']\n",
      "    ['Paul the Apostle', 'iconographic symbol', 'sword']\n",
      "    ['Agatha of Sicily', 'iconographic symbol', \"martyr's palm\"]\n",
      "    ['Linus', 'iconographic symbol', 'religious habit']\n",
      "    2 triples with sentences in 482.59 seconds!\n",
      "\n",
      "processing \"indigenous to\" (P2341) relation:\n",
      "  collected 7 triples\n",
      "  sample:\n",
      "    ['Ntukpo Dance', 'indigenous to', 'Igbo people']\n",
      "    ['Odegelu Dance', 'indigenous to', 'Igbo people']\n",
      "    ['Dance de Yebra de Basa', 'indigenous to', 'Yebra de Basa']\n",
      "    ['Dance de San Lorenzo', 'indigenous to', 'Huesca']\n",
      "    ['Igede and Igba-Udugongo.', 'indigenous to', 'Igbo people']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "Finished processing ontology ont_10_culture. Data saved.\n",
      "\n",
      "Ontology: Military Ontology (ont_5_military)\n",
      "\n",
      "processing \"military rank\" (P410) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Oskar Schwerk', 'military rank', 'Obergruppenführer']\n",
      "    ['Wilhelm Helten', 'military rank', 'Sturmscharführer']\n",
      "    ['Siegfried Taubert', 'military rank', 'Obergruppenführer']\n",
      "    ['Wilhelm Ohnesorge', 'military rank', 'Obergruppenführer']\n",
      "    ['Karl Fiehler', 'military rank', 'Obergruppenführer']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"military branch\" (P241) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Jorge Montt', 'military branch', 'Chilean Navy']\n",
      "    ['J. R. R. Tolkien', 'military branch', 'British Army']\n",
      "    ['George Orwell', 'military branch', 'International Brigades']\n",
      "    ['Nikita Khrushchev', 'military branch', 'Red Army']\n",
      "    ['Hassanal Bolkiah I of Brunei', 'military branch', 'Royal Air Force']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"military casualty classification \" (P1347) relation:\n",
      "  collected 6215 triples\n",
      "  sample:\n",
      "    ['Pyotr Makarov', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Hajo Rose', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Hasso von Manteuffel', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Hugh R. Miller', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Gustav Poel', 'military casualty classification ', 'prisoner of war']\n",
      "    200 triples with sentences in 0.04 seconds!\n",
      "\n",
      "processing \"designed by\" (P287) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "  collected 100 triples\n",
      "  sample:\n",
      "    ['8.35 cm PL kanon vz. 22', 'designed by', 'Škoda Works']\n",
      "    ['Ehrhardt 7.5 cm Model 1904', 'designed by', 'Rheinmetall AG']\n",
      "    ['28 cm K L/40 \"Kurfürst\"', 'designed by', 'Krupp']\n",
      "    ['17 cm Kanone (E)', 'designed by', 'Krupp']\n",
      "    ['15 cm hrubá houfnice vz. 25', 'designed by', 'Škoda Works']\n",
      "    7 triples with sentences in 60.18 seconds!\n",
      "\n",
      "processing \"designed by\" (P287) relation:\n",
      "  collected 4 triples\n",
      "  sample:\n",
      "    ['Pindad Komodo', 'designed by', 'Pindad']\n",
      "    ['M3 Bradley', 'designed by', 'FMC Corporation']\n",
      "    ['ASLAV', 'designed by', 'Mowag']\n",
      "    ['Puma armored engineering vehicle', 'designed by', 'Israel Military Industries']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"commanded by\" (P4791) relation:\n",
      "  collected 337 triples\n",
      "  sample:\n",
      "    ['Marineamt', 'commanded by', 'Albrecht Obermaier']\n",
      "    ['Army Office', 'commanded by', 'Horst Wenner']\n",
      "    ['Naval Medical Institute', 'commanded by', 'Armin Wandel']\n",
      "    ['Marineamt', 'commanded by', 'Jürgen Geier']\n",
      "    ['Army Office', 'commanded by', 'Hellmuth Mäder']\n",
      "    0 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"next higher rank\" (P3730) relation:\n",
      "  collected 447 triples\n",
      "  sample:\n",
      "    ['Scharführer', 'next higher rank', 'Oberscharführer']\n",
      "    ['SS-Brigadeführer', 'next higher rank', 'SS-Gruppenführer']\n",
      "    ['starshina', 'next higher rank', 'praporshchik']\n",
      "    ['Sturmscharführer', 'next higher rank', 'Untersturmführer']\n",
      "    ['Hai Jun Shao Jiang', 'next higher rank', 'Hai Jun Zhong Jiang']\n",
      "    7 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"designated as terrorist by\" (P3461) relation:\n",
      "  collected 234 triples\n",
      "  sample:\n",
      "    ['Babbar Khalsa', 'designated as terrorist by', 'India']\n",
      "    ['Continuity Irish Republican Army', 'designated as terrorist by', 'New Zealand']\n",
      "    [\"People's Mojahedin Organization of Iran\", 'designated as terrorist by', 'Iran']\n",
      "    ['Harkat-ul-Mujahideen', 'designated as terrorist by', 'Bahrain']\n",
      "    ['Harkat-ul-Mujahideen', 'designated as terrorist by', 'India']\n",
      "    24 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"wing configuration\" (P1654) relation:\n",
      "  collected 117 triples\n",
      "  sample:\n",
      "    ['Heinkel HD 27', 'wing configuration', 'biplane']\n",
      "    ['Focke-Wulf A 7', 'wing configuration', 'mid-wing aircraft']\n",
      "    ['BOK-5', 'wing configuration', 'low wing']\n",
      "    ['Friedrichshafen FF.34', 'wing configuration', 'biplane']\n",
      "    ['Aichi AB-6', 'wing configuration', 'biplane']\n",
      "    4 triples with sentences in 0.00 seconds!\n",
      "Finished processing ontology ont_5_military. Data saved.\n",
      "\n",
      "Ontology: Book Ontology (ont_4_book)\n",
      "\n",
      "processing \"illustrator\" (P110) relation:\n",
      "An error occurred: Invalid control character at: line 81563 column 86 (char 2063563). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 66165 column 102 (char 1670547). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['House of Mystery', 'illustrator', 'Tony Akins']\n",
      "    ['The Smurfs', 'illustrator', 'Alain Maury']\n",
      "    ['Hellblazer', 'illustrator', 'John Ridgway']\n",
      "    ['Thorgal', 'illustrator', 'Fred Vignaux']\n",
      "    ['House of Mystery', 'illustrator', 'Curt Swan']\n",
      "    200 triples with sentences in 130.51 seconds!\n",
      "\n",
      "processing \"followed by\" (P156) relation:\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 11120 column 8 (char 311190). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: IncompleteRead(0 bytes read). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: IncompleteRead(0 bytes read). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "An error occurred: Invalid control character at: line 11107 column 86 (char 311254). Retrying attempt 4/10 after short wait...\n",
      "Reducing LIMIT to 625 and retrying...\n",
      "  collected 625 triples\n",
      "  sample:\n",
      "    ['De concordia inter codices', 'followed by', 'Sedula Mater']\n",
      "    ['Lithuanian Constitution of 1922', 'followed by', 'Lithuanian Constitution of 1928']\n",
      "    ['Political Constitution of the State of Chile of 1822', 'followed by', 'Political Constitution of the State of Chile of 1823']\n",
      "    ['Provisional Constitution for the State of Chile of 1818', 'followed by', 'Political Constitution of the State of Chile of 1822']\n",
      "    ['Weimar Constitution', 'followed by', 'Constitution of the German Democratic Republic (1949)']\n",
      "    30 triples with sentences in 245.07 seconds!\n",
      "\n",
      "processing \"publication date\" (P577) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Kyokuhoku Rhapsody', 'publication date', '01 December 2011']\n",
      "    ['Of birds and beasts', 'publication date', '01 May 1935']\n",
      "    ['Of birds and beasts', 'publication date', '01 January 1933']\n",
      "    ['Naomi to Kanako', 'publication date', '12 November 2014']\n",
      "    ['Lemon', 'publication date', '15 May 1931']\n",
      "    89 triples with sentences in 0.10 seconds!\n",
      "\n",
      "processing \"author\" (P50) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Of birds and beasts', 'author', 'Yasunari Kawabata']\n",
      "    ['Set Theory', 'author', 'Thomas Jech']\n",
      "    ['Berkeley Physics Course. Mechanics', 'author', 'Charles Kittel']\n",
      "    ['Tokyo Tower', 'author', 'http://www.wikidata.org/entity/Q447073']\n",
      "    ['An Occurrence at Owl Creek Bridge One of the Missing', 'author', 'Ambrose Bierce']\n",
      "    200 triples with sentences in 0.04 seconds!\n",
      "\n",
      "processing \"publisher\" (P123) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['The Conquest of Plague: A Study of the Evolution of Epidemiology', 'publisher', 'Oxford University Press']\n",
      "    ['Positive computing: technology for wellbeing and human potential', 'publisher', 'The MIT Press']\n",
      "    ['The Oxford Handbook of Later Medieval Archaeology in Britain', 'publisher', 'Oxford University Press']\n",
      "    ['Magic Jewels of the Middle Ages and Renaissance', 'publisher', 'Oxford University Press']\n",
      "    ['The Oxford Handbook of the Archaeology of Childhood', 'publisher', 'Oxford University Press']\n",
      "    150 triples with sentences in 0.10 seconds!\n",
      "\n",
      "processing \"characters\" (P674) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['X-Men #6', 'characters', 'Beast']\n",
      "    ['Uncanny X-Men #194', 'characters', 'Wolverine']\n",
      "    ['X-Men #13', 'characters', 'Beast']\n",
      "    ['X-Men #7', 'characters', 'Beast']\n",
      "    ['X-Men #10', 'characters', 'Beast']\n",
      "    112 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"editor\" (P98) relation:\n",
      "  collected 4076 triples\n",
      "  sample:\n",
      "    ['Journal of Statistical Physics', 'editor', 'Joel Lebowitz']\n",
      "    ['Journal of Homosexuality', 'editor', 'Charles Silverstein']\n",
      "    [\"Bulletin of the British Ornithologists' Club\", 'editor', 'Percy Lowe']\n",
      "    ['Positif', 'editor', 'Michel Ciment']\n",
      "    ['Isis', 'editor', 'Bernard Lightman']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"place of publication\" (P291) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 17033 column 9 (char 433715). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 5393 column 6 (char 139071). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "An error occurred: Invalid control character at: line 5457 column 89 (char 139192). Retrying attempt 4/10 after short wait...\n",
      "Reducing LIMIT to 625 and retrying...\n",
      "  collected 625 triples\n",
      "  sample:\n",
      "    ['100 Jahre Eisenbahn Landshut – Vilsbiburg – Neumarkt-St. Veit', 'place of publication', 'Vilsbiburg']\n",
      "    ['Geographical statistical dictionary of the Russian Empire', 'place of publication', 'Saint Petersburg']\n",
      "    ['Shabdha Shodhini', 'place of publication', 'Thiruvananthapuram']\n",
      "    ['Die Hauptbahn München–Simbach und ihre Zweigbahnen', 'place of publication', 'Egglham']\n",
      "    ['First Malayalam-English Translator', 'place of publication', 'Mangaluru']\n",
      "    3 triples with sentences in 256.55 seconds!\n",
      "\n",
      "processing \"narrative location\" (P840) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "An error occurred: Invalid control character at: line 1643 column 87 (char 41029). Retrying attempt 5/10 after short wait...\n",
      "Reducing LIMIT to 312 and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "  collected 156 triples\n",
      "  sample:\n",
      "    ['La Belle Province', 'narrative location', 'Contrecœur']\n",
      "    ['Maus', 'narrative location', 'United States of America']\n",
      "    ['Teenage Mutant Ninja Turtles Meet the Conservation Corps', 'narrative location', 'Amazon rainforest']\n",
      "    ['Deadman Wonderland', 'narrative location', 'Japan']\n",
      "    ['Maus', 'narrative location', 'Sosnowiec Ghetto']\n",
      "    5 triples with sentences in 366.36 seconds!\n",
      "\n",
      "processing \"genre\" (P136) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "  collected 625 triples\n",
      "  sample:\n",
      "    ['The Closing Net', 'genre', 'thriller novel']\n",
      "    ['Nothing Lasts Forever', 'genre', 'thriller novel']\n",
      "    ['Lover', 'genre', 'lesbian literature']\n",
      "    ['Consolatio', 'genre', 'consolatio']\n",
      "    ['Owari meisho zue', 'genre', 'meisho zue']\n",
      "    7 triples with sentences in 240.75 seconds!\n",
      "\n",
      "processing \"language of work or name\" (P407) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Veni, veni Emmanuel', 'language of work or name', 'Latin']\n",
      "    ['Gnostic Apocalypse of Peter', 'language of work or name', 'Koine Greek']\n",
      "    ['Mortadelo', 'language of work or name', 'Spanish']\n",
      "    ['International Exhibition of Nothing', 'language of work or name', 'Italian']\n",
      "    ['The Futurist Architecture Manifesto', 'language of work or name', 'Italian']\n",
      "    38 triples with sentences in 0.07 seconds!\n",
      "\n",
      "processing \"depicts\" (P180) relation:\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 183917 column 2 (char 4861394). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    ['1Q84', 'depicts', 'NHK']\n",
      "    ['Abrahamic prayer', 'depicts', 'salah']\n",
      "    ['Abrahamic prayer', 'depicts', 'Sayyid']\n",
      "    ['Condorito', 'depicts', 'dog']\n",
      "    ['BE KOBE', 'depicts', 'K']\n",
      "    9 triples with sentences in 65.49 seconds!\n",
      "Finished processing ontology ont_4_book. Data saved.\n",
      "\n",
      "Ontology: Politics Ontology (ont_8_politics)\n",
      "\n",
      "processing \"head of state\" (P35) relation:\n",
      "  collected 244 triples\n",
      "  sample:\n",
      "    ['Italy', 'head of state', 'Sergio Mattarella']\n",
      "    ['Norway', 'head of state', 'Harald V of Norway']\n",
      "    ['Nepal', 'head of state', 'Ram Chandra Poudel']\n",
      "    ['Tuvalu', 'head of state', 'Charles III']\n",
      "    ['Finland', 'head of state', 'Alexander Stubb']\n",
      "    18 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"head of government\" (P6) relation:\n",
      "  collected 244 triples\n",
      "  sample:\n",
      "    ['Venezuela', 'head of government', 'Nicolás Maduro']\n",
      "    ['Slovakia', 'head of government', 'Robert Fico']\n",
      "    ['Bahrain', 'head of government', 'Salman bin Hamad, Crown Prince of Bahrain']\n",
      "    ['Hungary', 'head of government', 'Viktor Orbán']\n",
      "    ['Germany', 'head of government', 'Olaf Scholz']\n",
      "    4 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"position held\" (P39) relation:\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 96473 column 6 (char 2439196). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 51183 column 105 (char 1301400). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  2500 SPARQL results.\n",
      "Saved results to cache.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Federico Errázuriz Echaurren', 'position held', 'President of Chile']\n",
      "    ['Georges Pompidou', 'position held', 'President of the French Republic']\n",
      "    ['Jacques Chirac', 'position held', 'President of the French Republic']\n",
      "    ['Jorge Montt', 'position held', 'President of Chile']\n",
      "    ['François Mitterrand', 'position held', 'President of the French Republic']\n",
      "    200 triples with sentences in 178.93 seconds!\n",
      "\n",
      "processing \"Member of\" (P463) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "An error occurred: Invalid control character at: line 11227 column 139 (char 286629). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: Expecting ',' delimiter: line 12819 column 7 (char 327509). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "  collected 156 triples\n",
      "  sample:\n",
      "    ['Irina Beletskaya', 'Member of', 'Russian Academy of Sciences']\n",
      "    ['Charles Darwin', 'Member of', 'French Academy of Sciences']\n",
      "    ['Boutros Boutros-Ghali', 'Member of', 'Russian Academy of Sciences']\n",
      "    ['Carl Linnaeus', 'Member of', 'Royal Society']\n",
      "    [\"Marie-Jean-Léon d'Hervey de Saint Denys\", 'Member of', 'Russian Academy of Sciences']\n",
      "    4 triples with sentences in 371.85 seconds!\n",
      "\n",
      "processing \"political alignment\" (P1387) relation:\n",
      "  collected 3645 triples\n",
      "  sample:\n",
      "    ['New Serbia', 'political alignment', 'right-wing']\n",
      "    ['Slovak National Party', 'political alignment', 'right-wing']\n",
      "    ['New Serb Democracy', 'political alignment', 'right-wing']\n",
      "    ['Northern Ireland Unionist Party', 'political alignment', 'right-wing']\n",
      "    ['Popular Liberal Action', 'political alignment', 'right-wing']\n",
      "    1 triples with sentences in 0.03 seconds!\n",
      "\n",
      "processing \"member of political party\" (P102) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Guy Teissier', 'member of political party', 'Union for a Popular Movement']\n",
      "    ['John Major', 'member of political party', 'Conservative Party']\n",
      "    ['Nicolas Sarkozy', 'member of political party', 'Union for a Popular Movement']\n",
      "    ['Stephen Harper', 'member of political party', 'Liberal Party of Canada']\n",
      "    ['Manuel Montt', 'member of political party', 'Conservative Party']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"elected in\" (P2715) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 9/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 10/10. Reducing LIMIT and retrying...\n",
      "Max retries reached. Skipping this relation.\n",
      "    0 triples with sentences in 602.37 seconds!\n",
      "\n",
      "processing \"candidacy in election\" (P3602) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['John McCain', 'candidacy in election', '2008 United States presidential election']\n",
      "    ['David Cameron', 'candidacy in election', '2013 United Kingdom local elections']\n",
      "    ['Abraham Lincoln', 'candidacy in election', '1864 United States presidential election']\n",
      "    ['François Mitterrand', 'candidacy in election', '1974 French presidential election']\n",
      "    ['Nicolas Sarkozy', 'candidacy in election', '1999 European Parliament election']\n",
      "    3 triples with sentences in 0.09 seconds!\n",
      "\n",
      "processing \"political ideology\" (P1142) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    [\"Ploughmen's Front\", 'political ideology', 'left-wing populism']\n",
      "    ['National Progressive Unionist Party', 'political ideology', 'left-wing populism']\n",
      "    ['Nationalist Civic Crusade', 'political ideology', 'http://www.wikidata.org/entity/Q6072327']\n",
      "    ['Belarusian Socialist-Revolutionary Party', 'political ideology', 'left-wing populism']\n",
      "    ['Confederation of Anarcho-Syndicalists', 'political ideology', 'anarcho-syndicalism']\n",
      "    142 triples with sentences in 0.07 seconds!\n",
      "Finished processing ontology ont_8_politics. Data saved.\n",
      "\n",
      "Ontology: Computer Ontology (ont_6_computer)\n",
      "\n",
      "processing \"developer\" (P178) relation:\n",
      "An error occurred: Expecting ',' delimiter: line 58504 column 2 (char 1458033). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 64441 column 5 (char 1605485). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Borland Together', 'developer', 'Borland']\n",
      "    ['Netscape Navigator 4.05', 'developer', 'Netscape']\n",
      "    ['Internet Channel', 'developer', 'Opera Software']\n",
      "    ['Firefox Extended Support Release', 'developer', 'Mozilla Corporation']\n",
      "    ['Borland Graphics Interface', 'developer', 'Borland']\n",
      "    200 triples with sentences in 130.61 seconds!\n",
      "\n",
      "processing \"creator\" (P170) relation:\n",
      "  collected 372 triples\n",
      "  sample:\n",
      "    ['vi', 'creator', 'Bill Joy']\n",
      "    ['emacs-code-cells', 'creator', 'Augusto Stoffel']\n",
      "    ['emacs-osm', 'creator', 'Daniel Mendler']\n",
      "    ['ed', 'creator', 'Ken Thompson']\n",
      "    ['fish', 'creator', 'Axel Liljencrantz']\n",
      "    22 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"platform\" (P400) relation:\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 123944 column 7 (char 3071861). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    ['IBM DisplayWrite', 'platform', 'IBM Personal Computer']\n",
      "    ['TVEDIT', 'platform', 'PDP-1']\n",
      "    ['CEMM', 'platform', 'IBM PC compatible']\n",
      "    ['Cubic IDE', 'platform', 'Commodore Amiga']\n",
      "    ['VisiCalc', 'platform', 'Apple II']\n",
      "    200 triples with sentences in 65.73 seconds!\n",
      "\n",
      "processing \"operating system\" (P306) relation:\n",
      "  collected 1868 triples\n",
      "  sample:\n",
      "    ['FM Towns', 'operating system', 'Gentoo Linux']\n",
      "    ['Nokia 5230', 'operating system', 'Symbian']\n",
      "    ['Nokia 5530 XpressMusic', 'operating system', 'Symbian']\n",
      "    ['Nokia E71', 'operating system', 'Symbian']\n",
      "    ['Nokia 6630', 'operating system', 'Symbian']\n",
      "    53 triples with sentences in 0.02 seconds!\n",
      "Finished processing ontology ont_6_computer. Data saved.\n",
      "\n",
      "Ontology: Movie Ontology (ont_1_movie)\n",
      "\n",
      "processing \"director\" (P57) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Pilot', 'director', 'Jace Alexander']\n",
      "    ['Karas', 'director', 'Keiichi Satō']\n",
      "    ['Help Wanted', 'director', 'Stephen Hillenburg']\n",
      "    ['Green Vs. Red', 'director', 'Shigeyuki Miya']\n",
      "    ['Key the Metal Idol', 'director', 'Yorifusa Yamaguchi']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"screenwriter\" (P58) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Help Wanted', 'screenwriter', 'Stephen Hillenburg']\n",
      "    ['Help Wanted', 'screenwriter', 'Tim Hill']\n",
      "    ['The Life of Budori Gusuko', 'screenwriter', 'Gisaburō Sugii']\n",
      "    ['Locke the Superman', 'screenwriter', 'Atsushi Yamatoya']\n",
      "    ['Pilot', 'screenwriter', 'Brad Falchuk']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"genre\" (P136) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['The Rescuers', 'genre', 'film based on literature']\n",
      "    ['The Little Fox', 'genre', 'film based on literature']\n",
      "    ['Tatarak', 'genre', 'film based on literature']\n",
      "    ['Mutiny on the Bounty', 'genre', 'film based on literature']\n",
      "    ['Winnie-the-Pooh', 'genre', 'film based on literature']\n",
      "    2 triples with sentences in 0.07 seconds!\n",
      "\n",
      "processing \"based on\" (P144) relation:\n",
      "An error occurred: Invalid control character at: line 50855 column 86 (char 1285921). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Expecting ',' delimiter: line 18917 column 1 (char 483109). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Fate/stay night: Unlimited Blade Works', 'based on', 'Fate/stay night']\n",
      "    ['Anne of Green Gables', 'based on', 'Anne of Green Gables']\n",
      "    [\"Book Girl Today's Snack: First Love\", 'based on', 'Book Girl']\n",
      "    ['Harlock: Space Pirate', 'based on', 'Captain Harlock']\n",
      "    [\"Kino's Journey: Life Goes On\", 'based on', \"Kino's Journey\"]\n",
      "    146 triples with sentences in 131.68 seconds!\n",
      "\n",
      "processing \"cast member\" (P161) relation:\n",
      "  10000 SPARQL results.\n",
      "Saved results to cache.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Zwei Weihnachtsmänner', 'cast member', 'Armin Rohde']\n",
      "    ['Zwei Weihnachtsmänner', 'cast member', 'Lotte Ledl']\n",
      "    ['Aquaman', 'cast member', 'Ving Rhames']\n",
      "    ['Tsunami: The Aftermath', 'cast member', 'Tim Roth']\n",
      "    ['Pilot', 'cast member', 'Dominic Monaghan']\n",
      "    200 triples with sentences in 33.21 seconds!\n",
      "\n",
      "processing \"award received\" (P166) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Casablanca', 'award received', 'Academy Award for Best Director']\n",
      "    ['Walk the Line', 'award received', 'Academy Award for Best Actress']\n",
      "    ['Nights of Cabiria', 'award received', 'Academy Award for Best International Feature Film']\n",
      "    ['Forrest Gump', 'award received', 'Academy Award for Best Director']\n",
      "    ['8½', 'award received', 'Academy Award for Best International Feature Film']\n",
      "    200 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"production company\" (P272) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['The Prey of the Wind', 'production company', 'Films Albatros']\n",
      "    [\"Halal police d'État\", 'production company', '4 Mecs en Baskets']\n",
      "    ['Brides', 'production company', 'Adastra Films']\n",
      "    ['The Crimson Ghost', 'production company', 'Republic Pictures']\n",
      "    ['Flame of Barbary Coast', 'production company', 'Republic Pictures']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"country of origin\" (P495) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['The Nutcracker Prince', 'country of origin', 'Canada']\n",
      "    ['Rise of the Gargoyles', 'country of origin', 'Canada']\n",
      "    ['Khalass', 'country of origin', 'Lebanon']\n",
      "    ['Melodrama Habibi', 'country of origin', 'Lebanon']\n",
      "    ['Bye Bye London', 'country of origin', 'Kuwait']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"publication date\" (P577) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Kimagure Robot', 'publication date', '01 January 2004']\n",
      "    ['Joe vs. Joe', 'publication date', '01 January 2003']\n",
      "    ['Submarine 707R', 'publication date', '01 January 2003']\n",
      "    ['Yawaraka Sangokushi Tsukisase!! Ryofuko-chan', 'publication date', '01 January 2007']\n",
      "    ['Maple Colors', 'publication date', '01 January 2004']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"characters\" (P674) relation:\n",
      "  collected 3960 triples\n",
      "  sample:\n",
      "    ['Batman Forever', 'characters', 'Batman']\n",
      "    ['The Batman', 'characters', 'Batman']\n",
      "    ['The Count of Monte Cristo', 'characters', 'Fernand Mondego']\n",
      "    ['The Count of Monte Cristo', 'characters', 'Abbe Faria']\n",
      "    ['Batman & Robin', 'characters', 'Batman']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"narrative location\" (P840) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Ride Your Wave', 'narrative location', 'Chiba']\n",
      "    ['Nalai Manithan', 'narrative location', 'Chennai']\n",
      "    ['Her Blue Sky', 'narrative location', 'Chichibu']\n",
      "    ['Crayon Shin-chan: Fierceness That Invites Storm! Yakiniku Road of Honor', 'narrative location', 'Atami']\n",
      "    ['The Man with the Iron Heart', 'narrative location', 'Prague']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"filming location\" (P915) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Make It Count', 'filming location', 'Bangkok']\n",
      "    ['Gunner', 'filming location', 'Bessemer']\n",
      "    ['Make It Count', 'filming location', 'Singapore']\n",
      "    ['Gunner', 'filming location', 'Birmingham']\n",
      "    ['Victim', 'filming location', 'Mount Pearl']\n",
      "    200 triples with sentences in 0.03 seconds!\n",
      "\n",
      "processing \"main subject\" (P921) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Strawberry Marshmallow', 'main subject', 'elementary school student']\n",
      "    ['Rendite statt Respekt — Wenn Arbeit ihren Wert verliert', 'main subject', 'minimum wage']\n",
      "    ['Rendite statt Respekt — Wenn Arbeit ihren Wert verliert', 'main subject', 'social justice']\n",
      "    [\"Howl's Moving Castle\", 'main subject', 'war']\n",
      "    [\"Loadsamoney (Doin' Up the House)\", 'main subject', 'wealth']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"nominated for\" (P1411) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['When Marnie Was There', 'nominated for', 'Academy Award for Best Animated Feature']\n",
      "    ['8½', 'nominated for', 'Academy Award for Best Director']\n",
      "    ['The Piano', 'nominated for', 'BAFTA Award for Best Film']\n",
      "    ['The Wind Rises', 'nominated for', 'Academy Award for Best Animated Feature']\n",
      "    ['Silver Linings Playbook', 'nominated for', 'Academy Award for Best Supporting Actress']\n",
      "    196 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"cost\" (P2130) relation:\n",
      "  collected 6370 triples\n",
      "  sample:\n",
      "    ['E.T. the Extra-Terrestrial', 'cost', '10500000']\n",
      "    ['From Selma to New York', 'cost', '100000']\n",
      "    ['V for Vendetta', 'cost', '54000000']\n",
      "    ['Eraserhead', 'cost', '100000']\n",
      "    ['Dances with Wolves', 'cost', '22000000']\n",
      "    0 triples with sentences in 0.04 seconds!\n",
      "Finished processing ontology ont_1_movie. Data saved.\n",
      "\n",
      "Ontology: Sports Ontology (ont_3_sport)\n",
      "\n",
      "processing \"occupation\" (P106) relation:\n",
      "An error occurred: Invalid control character at: line 174633 column 109 (char 4364539). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    ['Alou Diarra', 'occupation', 'association football player']\n",
      "    ['Andre de Kruijff', 'occupation', 'association football player']\n",
      "    ['Jérémy Toulalan', 'occupation', 'association football player']\n",
      "    ['Karim Benzema', 'occupation', 'association football player']\n",
      "    ['Mathieu Valbuena', 'occupation', 'association football player']\n",
      "    200 triples with sentences in 65.26 seconds!\n",
      "\n",
      "processing \"sport\" (P641) relation:\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 36023 column 1 (char 900984). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 75685 column 86 (char 1947861). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  2500 SPARQL results.\n",
      "Saved results to cache.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['2008 Grand Slam of Darts', 'sport', 'darts']\n",
      "    ['2005 World Matchplay', 'sport', 'darts']\n",
      "    ['Las Vegas Desert Classic', 'sport', 'darts']\n",
      "    ['Dutch Open', 'sport', 'darts']\n",
      "    ['WDF World Cup', 'sport', 'darts']\n",
      "    200 triples with sentences in 175.34 seconds!\n",
      "\n",
      "processing \"member of sports team\" (P54) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Abou Diaby', 'member of sports team', 'AJ Auxerre']\n",
      "    ['Abou Diaby', 'member of sports team', 'Olympique de Marseille']\n",
      "    ['Franck Ribéry', 'member of sports team', 'U.S. Salernitana 1919']\n",
      "    ['Zinedine Zidane', 'member of sports team', 'FC Girondins de Bordeaux']\n",
      "    ['Zinedine Zidane', 'member of sports team', 'Juventus FC']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"country for sport\" (P1532) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Sengphachan Bounthisanh', 'country for sport', 'Laos']\n",
      "    ['Jasem Yaqoub', 'country for sport', 'Kuwait']\n",
      "    ['Poh Chin Peng', 'country for sport', 'Malaysia']\n",
      "    ['Chen Tang Jie', 'country for sport', 'Malaysia']\n",
      "    ['Thinaah Muralitharan', 'country for sport', 'Malaysia']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"sports season of league or competition\" (P3450) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['1984–85 2. Fußball-Bundesliga', 'sports season of league or competition', '2. Bundesliga']\n",
      "    ['2004–05 Four Hills Tournament', 'sports season of league or competition', 'Four Hills Tournament']\n",
      "    ['2010–11 2. Fußball-Bundesliga', 'sports season of league or competition', '2. Frauen-Bundesliga']\n",
      "    ['2014 IIHF World Championship', 'sports season of league or competition', 'Ice Hockey World Championships']\n",
      "    [\"2016 Men's World Ice Hockey Championships\", 'sports season of league or competition', 'Ice Hockey World Championships']\n",
      "    112 triples with sentences in 0.12 seconds!\n",
      "\n",
      "processing \"coach of sports team\" (P6087) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Diego Maradona', 'coach of sports team', 'Racing Club de Avellaneda']\n",
      "    ['Markus Weinzierl', 'coach of sports team', 'FC Augsburg']\n",
      "    ['Óscar Tabárez', 'coach of sports team', 'AC Milan']\n",
      "    ['Óscar Tabárez', 'coach of sports team', 'C.A. Bella Vista']\n",
      "    ['Piet Kraak', 'coach of sports team', 'Velox']\n",
      "    57 triples with sentences in 0.10 seconds!\n",
      "\n",
      "processing \"league\" (P118) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Craig Gardner', 'league', 'Premier League']\n",
      "    ['David Meyler', 'league', 'Premier League']\n",
      "    ['Michael Owen', 'league', 'Premier League']\n",
      "    ['Andre Wisdom', 'league', 'Premier League']\n",
      "    ['Luke Wilkshire', 'league', 'Premier League']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"home venue\" (P159) relation:\n",
      "  collected 71 triples\n",
      "  sample:\n",
      "    ['FC BFV', 'home venue', 'Mahamasina Municipal Stadium']\n",
      "    ['Football Club Côtière Luenaz', 'home venue', 'Stade des Gravelles']\n",
      "    ['Club Vasconia', 'home venue', \"Pavelló de la Vall d'Hebron\"]\n",
      "    ['Stade bordelais', 'home venue', 'Stade Sainte-Germain']\n",
      "    ['Red Star F.C.', 'home venue', 'Stade de Paris']\n",
      "    3 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"country of origin \" (P495) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "  collected 39 triples\n",
      "  sample:\n",
      "    ['Chinlone', 'country of origin ', 'Myanmar']\n",
      "    ['Yaniv', 'country of origin ', 'Nepal']\n",
      "    ['3-2-5', 'country of origin ', 'Pakistan']\n",
      "    ['Lethwei', 'country of origin ', 'Myanmar']\n",
      "    ['short-track speed skating', 'country of origin ', 'Canada']\n",
      "    3 triples with sentences in 483.00 seconds!\n",
      "\n",
      "processing \"league\" (P118) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Gerard Dicaire', 'league', 'ECHL']\n",
      "    ['Kalvin Phillips', 'league', 'English Football League']\n",
      "    ['Jack Grealish', 'league', 'English Football League']\n",
      "    ['Kenny Dennard', 'league', 'Continental Basketball Association']\n",
      "    ['Brady Austin', 'league', 'ECHL']\n",
      "    200 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"competition class\" (P2094) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Boston Breakers', 'competition class', \"women's association football\"]\n",
      "    ['London Bees', 'competition class', \"women's association football\"]\n",
      "    ['TuS Köln rrh.', 'competition class', \"women's association football\"]\n",
      "    ['Boston Breakers', 'competition class', \"women's association football\"]\n",
      "    ['FC Bayern Munich Women', 'competition class', \"women's association football\"]\n",
      "    148 triples with sentences in 0.10 seconds!\n",
      "Finished processing ontology ont_3_sport. Data saved.\n",
      "\n",
      "Ontology: Space Ontology (ont_7_space)\n",
      "\n",
      "processing \"site of astronomical discovery\" (P65) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['(7609) 1995 WX3', 'site of astronomical discovery', 'Nachi-Katsuura Observatory']\n",
      "    ['7533 Seiraiji', 'site of astronomical discovery', 'Nachi-Katsuura Observatory']\n",
      "    ['(9970) 1992 ST1', 'site of astronomical discovery', 'Dainik Astronomical Observatory']\n",
      "    ['306019 Duren', 'site of astronomical discovery', 'Wide-field Infrared Survey Explorer']\n",
      "    ['439718 Danielcervantes', 'site of astronomical discovery', 'Wide-field Infrared Survey Explorer']\n",
      "    32 triples with sentences in 0.11 seconds!\n",
      "\n",
      "processing \"minor planet group\" (P196) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 9/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 10/10. Reducing LIMIT and retrying...\n",
      "Max retries reached. Skipping this relation.\n",
      "    0 triples with sentences in 602.15 seconds!\n",
      "\n",
      "processing \"constellation\" (P59) relation:\n",
      "  collected 1772 triples\n",
      "  sample:\n",
      "    ['NGC 5639', 'constellation', 'Boötes']\n",
      "    ['NGC 5888', 'constellation', 'Boötes']\n",
      "    ['NGC 4108', 'constellation', 'Draco']\n",
      "    ['NGC 4238', 'constellation', 'Draco']\n",
      "    ['NGC 4332', 'constellation', 'Draco']\n",
      "    126 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"astronaut mission\" (P450) relation:\n",
      "  collected 1798 triples\n",
      "  sample:\n",
      "    ['Hans Schlegel', 'astronaut mission', 'STS-55']\n",
      "    ['Sergei Krikalev', 'astronaut mission', 'STS-60']\n",
      "    ['Sergei Krikalev', 'astronaut mission', 'Soyuz TM-13']\n",
      "    ['Mark Shuttleworth', 'astronaut mission', 'Soyuz TM-33']\n",
      "    ['Naoko Yamazaki', 'astronaut mission', 'STS-131']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"spacecraft docking/undocking date\" (P622) relation:\n",
      "  collected 63 triples\n",
      "  sample:\n",
      "    ['Soyuz TMA-18M', 'spacecraft docking/undocking date', '04 September 2015']\n",
      "    ['Cygnus NG-11', 'spacecraft docking/undocking date', '19 April 2019']\n",
      "    ['Cygnus NG-10', 'spacecraft docking/undocking date', '19 November 2018']\n",
      "    ['Soyuz TMA-18M', 'spacecraft docking/undocking date', '02 March 2016']\n",
      "    ['Cygnus NG-11', 'spacecraft docking/undocking date', '06 August 2019']\n",
      "    15 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"backup or reserve team or crew\" (P3015) relation:\n",
      "  collected 236 triples\n",
      "  sample:\n",
      "    ['Gemini 5', 'backup or reserve team or crew', 'Neil Armstrong']\n",
      "    ['Gemini 9A', 'backup or reserve team or crew', 'Jim Lovell']\n",
      "    ['Soyuz 29', 'backup or reserve team or crew', 'Viktor Gorbatko']\n",
      "    ['Soyuz MS-18', 'backup or reserve team or crew', 'Anne McClain']\n",
      "    ['Soyuz T-7', 'backup or reserve team or crew', 'Vladimir Vasyutin']\n",
      "    2 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"location of landing\" (P1158) relation:\n",
      "  collected 188 triples\n",
      "  sample:\n",
      "    ['Columbia', 'location of landing', 'North Pacific Ocean']\n",
      "    ['Apollo 14', 'location of landing', 'Pacific Ocean']\n",
      "    ['Apollo 16', 'location of landing', 'Pacific Ocean']\n",
      "    ['Low-Earth Orbit Flight Test of an Inflatable Decelerator', 'location of landing', 'Pacific Ocean']\n",
      "    ['Biosatellite 3', 'location of landing', 'Pacific Ocean']\n",
      "    22 triples with sentences in 0.00 seconds!\n",
      "Finished processing ontology ont_7_space. Data saved.\n",
      "\n",
      "Ontology: Nature Ontology (ont_9_nature)\n",
      "\n",
      "processing \"parent taxon\" (P171) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Ornithopodichnites', 'parent taxon', 'Ornithopoda']\n",
      "    ['Bonaparteichnium', 'parent taxon', 'Ornithopoda']\n",
      "    ['Minisauripus', 'parent taxon', 'Dromaeopodidae']\n",
      "    ['Hadrosaurichnus', 'parent taxon', 'dinosaur']\n",
      "    ['Eubrontes', 'parent taxon', 'Eubrontidae']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"mountain range\" (P4552) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Weißer Stein', 'mountain range', 'Eifel']\n",
      "    ['Szeroka', 'mountain range', 'Sudetes']\n",
      "    ['Milseburg', 'mountain range', 'Rhön Mountains']\n",
      "    ['Suchawa', 'mountain range', 'Sudetes']\n",
      "    ['Strážná', 'mountain range', 'Sudetes']\n",
      "    200 triples with sentences in 0.03 seconds!\n",
      "\n",
      "processing \"parent peak\" (P3137) relation:\n",
      "  collected 2710 triples\n",
      "  sample:\n",
      "    ['Basòdino', 'parent peak', 'Dammastock']\n",
      "    ['Mönch', 'parent peak', 'Finsteraarhorn']\n",
      "    [\"L'Argentine\", 'parent peak', 'Finsteraarhorn']\n",
      "    ['Titlis', 'parent peak', 'Dammastock']\n",
      "    ['Croix de Fer', 'parent peak', 'Mont Blanc']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"taxon common name\" (P1843) relation:\n",
      "  collected 250 triples\n",
      "  sample:\n",
      "    ['Panthera uncia', 'taxon common name', 'snežni leopard']\n",
      "    ['Panthera uncia', 'taxon common name', '雪豹']\n",
      "    ['Chinese sturgeon', 'taxon common name', 'jeseter čínský']\n",
      "    ['Chinese sturgeon', 'taxon common name', 'Chinesischer Stör']\n",
      "    ['Chinese sturgeon', 'taxon common name', 'Chinese sturgeon']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"tributary\" (P974) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Nahal Taninim', 'tributary', 'Naẖal Allona']\n",
      "    ['Oder', 'tributary', 'Luha']\n",
      "    ['Oder', 'tributary', 'Barycz']\n",
      "    ['Vistula', 'tributary', 'Zgłowiączka']\n",
      "    ['Vistula', 'tributary', 'Świder']\n",
      "    101 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"origin of the watercourse\" (P885) relation:\n",
      "An error occurred: Expecting value: line 80882 column 15 (char 2004737). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Expecting ',' delimiter: line 67720 column 26 (char 1677431). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Ottawa River', 'origin of the watercourse', 'Lac des Outaouais']\n",
      "    ['Harricana River', 'origin of the watercourse', 'Lac Blouin']\n",
      "    ['Rönne River', 'origin of the watercourse', 'Ringsjön']\n",
      "    ['Jubba River', 'origin of the watercourse', 'Dawa River']\n",
      "    ['Jubba River', 'origin of the watercourse', 'Ganale Dorya River']\n",
      "    118 triples with sentences in 130.86 seconds!\n",
      "\n",
      "processing \"mouth of the watercourse\" (P403) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Naẖal Kesalon', 'mouth of the watercourse', 'Nahal Sorek']\n",
      "    ['Naẖal Mivrakh', 'mouth of the watercourse', 'Naẖal Asaf']\n",
      "    [\"Nahal Bar'am\", 'mouth of the watercourse', 'Nahal Dishon']\n",
      "    ['Naẖal Moresan', 'mouth of the watercourse', 'Naẖal H̱illazon']\n",
      "    ['Naẖal Betarim', 'mouth of the watercourse', 'Nahal Hevron']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"IUCN conservation status\" (P141) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Black Redstart', 'IUCN conservation status', 'Least Concern']\n",
      "    ['caracal', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Common Redstart', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Common Goldeneye', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Clouded apollo', 'IUCN conservation status', 'Least Concern']\n",
      "    0 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"taxon synonym\" (P1420) relation:\n",
      "  collected 4284 triples\n",
      "  sample:\n",
      "    ['Nanochromis dimidiatus', 'taxon synonym', 'Congochromis dimidiatus']\n",
      "    ['Torricelliaceae', 'taxon synonym', 'Aralidiaceae']\n",
      "    ['Arabian carpetshark', 'taxon synonym', 'Chiloscyllium confusum']\n",
      "    ['Ophiomitrella porrecta', 'taxon synonym', 'Ophiacantha porrecta']\n",
      "    ['Myrtaceae', 'taxon synonym', 'Psiloxylaceae']\n",
      "    93 triples with sentences in 0.03 seconds!\n",
      "\n",
      "processing \"reservoir created\" (P4661) relation:\n",
      "  collected 3084 triples\n",
      "  sample:\n",
      "    ['Sardis Dam', 'reservoir created', 'Sardis Lake']\n",
      "    ['Crystal Springs Dam', 'reservoir created', 'Crystal Springs Reservoir']\n",
      "    ['Kariba Dam', 'reservoir created', 'Lake Kariba']\n",
      "    ['Sam Rayburn Dam', 'reservoir created', 'Sam Rayburn Reservoir']\n",
      "    ['Dongjiang Dam', 'reservoir created', 'Dongjiang Lake']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"drainage basin\" (P4614) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Kalix River', 'drainage basin', 'http://www.wikidata.org/entity/Q65236365']\n",
      "    ['Pakanajoki', 'drainage basin', 'Uutuanjoki basin']\n",
      "    ['Ruisseau Veilleux', 'drainage basin', 'Restigouche River basin']\n",
      "    ['Río Manso', 'drainage basin', 'Puelo Basin']\n",
      "    ['Rivière Assemetquagan', 'drainage basin', 'Restigouche River basin']\n",
      "    46 triples with sentences in 0.10 seconds!\n",
      "\n",
      "processing \"mountains classification \" (P4320) relation:\n",
      "  collected 8819 triples\n",
      "  sample:\n",
      "    ['2003 Tour de Suisse', 'mountains classification ', 'Giuseppe Guerini']\n",
      "    ['2006 Tour de Suisse', 'mountains classification ', 'Marco Fertonani']\n",
      "    ['2006 Tour de Suisse', 'mountains classification ', 'Martin Elmiger']\n",
      "    ['2004 Tour de Suisse', 'mountains classification ', 'Robert Hunter']\n",
      "    ['2004 Tour de Suisse', 'mountains classification ', 'Michael Blaudzun']\n",
      "    131 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"Habitat\" (P2974) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "  collected 88 triples\n",
      "  sample:\n",
      "    ['Javan Pond Heron', 'Habitat', 'wetland']\n",
      "    ['Calluna vulgaris', 'Habitat', 'bog']\n",
      "    ['Arundinax aedon', 'Habitat', 'wetland']\n",
      "    ['chum salmon', 'Habitat', 'North Pacific Ocean']\n",
      "    ['Pink-breasted Flowerpecker', 'Habitat', 'wetland']\n",
      "    1 triples with sentences in 181.00 seconds!\n",
      "Finished processing ontology ont_9_nature. Data saved.\n",
      "\n",
      "Ontology: Music Ontology (ont_2_music)\n",
      "\n",
      "processing \"composer\" (P86) relation:\n",
      "  10000 SPARQL results.\n",
      "Saved results to cache.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Good Times', 'composer', 'Nile Rodgers']\n",
      "    [\"'Till I Collapse\", 'composer', 'Eminem']\n",
      "    ['Signs', 'composer', 'Snoop Dogg']\n",
      "    ['Somewhere I Belong', 'composer', 'Brad Delson']\n",
      "    ['Somewhere I Belong', 'composer', 'Mike Shinoda']\n",
      "    200 triples with sentences in 41.32 seconds!\n",
      "\n",
      "processing \"part of\" (P361) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['I Like', 'part of', 'In a Perfect World…']\n",
      "    ['Business', 'part of', 'The Eminem Show']\n",
      "    ['All the Things She Said', 'part of', '200 km/h in the Wrong Lane']\n",
      "    ['Vision of Love', 'part of', 'Mariah Carey']\n",
      "    [\"If I Ain't Got You\", 'part of', 'The Diary of Alicia Keys']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"lyrics by\" (P676) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Two Hearts', 'lyrics by', 'Phil Collins']\n",
      "    ['A Pillow of Winds', 'lyrics by', 'Roger Waters']\n",
      "    [\"Wavin' Flag\", 'lyrics by', 'Bruno Mars']\n",
      "    ['Disturbia', 'lyrics by', 'Rihanna']\n",
      "    ['AKA... What a Life!', 'lyrics by', 'Noel Gallagher']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"publication date\" (P577) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Man gewöhnt sich so schnell an das Schöne', 'publication date', '01 January 1964']\n",
      "    ['Addicted', 'publication date', '01 January 2006']\n",
      "    ['Runaway', 'publication date', '24 October 2000']\n",
      "    ['Safe & Sound', 'publication date', '23 December 2011']\n",
      "    ['Calling All Girls', 'publication date', '01 January 1982']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"language of work or name\" (P407) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Spem in alium', 'language of work or name', 'Latin']\n",
      "    ['Veni, veni Emmanuel', 'language of work or name', 'Latin']\n",
      "    ['Barcelona', 'language of work or name', 'Spanish']\n",
      "    ['Sesostri', 'language of work or name', 'Italian']\n",
      "    ['Fuerte', 'language of work or name', 'Spanish']\n",
      "    13 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"voice type\" (P412) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Domenico Cosselli', 'voice type', 'bass']\n",
      "    ['Natale de Carolis', 'voice type', 'bass']\n",
      "    ['Franz Kalchmair', 'voice type', 'bass']\n",
      "    ['Jean-Baptiste Pierné', 'voice type', 'bass']\n",
      "    ['Nikos Zaccaria', 'voice type', 'bass']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"instrumentation\" (P870) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Songs from the Basewood', 'instrumentation', 'guitar']\n",
      "    ['Aus Chinesich-deutsche Jahres- und Tageszeiten', 'instrumentation', 'voice']\n",
      "    ['If Ye Love Me', 'instrumentation', 'SATB choir']\n",
      "    ['Ach mein herzliebes Jesulein', 'instrumentation', 'string section']\n",
      "    ['Concerto for Clarinet', 'instrumentation', 'jazz band']\n",
      "    200 triples with sentences in 0.03 seconds!\n",
      "\n",
      "processing \"tracklist\" (P658) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    [\"(What's the Story) Morning Glory?\", 'tracklist', 'Morning Glory']\n",
      "    ['The Lady Killer', 'tracklist', 'Fuck You']\n",
      "    ['Nevermind', 'tracklist', 'Lithium']\n",
      "    ['Born to Die', 'tracklist', 'Blue Jeans']\n",
      "    ['Born to Die', 'tracklist', 'Off to the Races']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"genre\" (P136) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Ko To No O To', 'genre', 'noise music']\n",
      "    ['F.I.D.', 'genre', 'noise music']\n",
      "    ['Venez donc chez moi', 'genre', 'Gypsy jazz']\n",
      "    ['Hiranya', 'genre', 'noise music']\n",
      "    ['Material Action 2 N.A.M.', 'genre', 'noise music']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"performer\" (P175) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['H•A•M', 'performer', 'Kanye West']\n",
      "    ['Who Wants to Live Forever', 'performer', 'Queen']\n",
      "    ['Touch My Body', 'performer', 'Mariah Carey']\n",
      "    ['Someday', 'performer', 'Mariah Carey']\n",
      "    ['Bang!', 'performer', 'After School']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"producer\" (P162) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Strawberries Oceans Ships Forest', 'producer', 'Paul McCartney']\n",
      "    ['Rushes', 'producer', 'Paul McCartney']\n",
      "    ['Off the Wall', 'producer', 'Quincy Jones']\n",
      "    ['Das ist nicht die ganze Wahrheit...', 'producer', 'Uwe Hoffmann']\n",
      "    ['Best of The Corrs', 'producer', 'David Foster']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"nominated for\" (P1411) relation:\n",
      "  collected 419 triples\n",
      "  sample:\n",
      "    ['Come Around Sundown', 'nominated for', 'Grammy Award for Best Rock Album']\n",
      "    ['Justified', 'nominated for', 'Grammy Award for Album of the Year']\n",
      "    ['Return of Saturn', 'nominated for', 'Grammy Award for Best Rock Album']\n",
      "    ['Black Ice', 'nominated for', 'Grammy Award for Best Rock Album']\n",
      "    ['Judy at Carnegie Hall', 'nominated for', 'Grammy Award for Album of the Year']\n",
      "    24 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"record label\" (P264) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Opel-Gang', 'record label', 'EMI']\n",
      "    ['9', 'record label', 'BMG Rights Management']\n",
      "    ['American Dream', 'record label', 'Atlantic Records']\n",
      "    ['Blood on the Tracks', 'record label', 'Columbia Records']\n",
      "    ['Merzbow / Sutcliffe Jügend / Satori', 'record label', 'Cold Spring']\n",
      "    200 triples with sentences in 0.01 seconds!\n",
      "Finished processing ontology ont_2_music. Data saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update base path to use existing ontology directory\n",
    "base_path = '../../data/wikidata_tekgen/ontologies'\n",
    "\n",
    "# Check if directory exists, if not create it\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    error_message = \"\"\"\n",
    "ERROR: Ontology files not found!\n",
    "Please copy the original ontology files from the Text2KGBench repository\n",
    "\"\"\".format(base_path)\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Check if directory is empty or missing ontology files\n",
    "ontology_files = [f for f in os.listdir(base_path) if f.endswith('_ontology.json')]\n",
    "if not ontology_files:\n",
    "    error_message = \"\"\"\n",
    "ERROR: Ontology files not found!\n",
    "Please copy the original ontology files from the Text2KGBench repository\n",
    "\"\"\".format(base_path)\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load existing ontologies\n",
    "ontologies = []\n",
    "for filename in os.listdir(base_path):\n",
    "    if filename.endswith('_ontology.json'):\n",
    "        with open(os.path.join(base_path, filename)) as in_file:\n",
    "            ontologies.append(json.load(in_file))\n",
    "\n",
    "# Main flags\n",
    "show_sample = True\n",
    "show_query = False\n",
    "\n",
    "for onto in ontologies:\n",
    "    print(f\"Ontology: {onto['title']} ({onto['id']})\")\n",
    "    onto_id = onto['id']\n",
    "    \n",
    "    # Reset accumulators for each ontology\n",
    "    train_all, val_all, test_all = [], [], []\n",
    "    \n",
    "    for rel in onto['relations']:\n",
    "        print(f\"\\nprocessing \\\"{rel['label']}\\\" ({rel['pid']}) relation:\")\n",
    "        start_time = time.time()\n",
    "        triples_with_sentences = get_triples_with_sentences(\n",
    "            rel['pid'], rel['label'], rel['domain'], rel['range'], limit=200, max_retries=10)\n",
    "        elapsed_time = (time.time() - start_time)\n",
    "        print(f\"    {len(triples_with_sentences)} triples with sentences in {elapsed_time:.2f} seconds!\")\n",
    "        train, val, test = get_splits(triples_with_sentences)\n",
    "        train_all += train\n",
    "        val_all += val\n",
    "        test_all += test\n",
    "    \n",
    "    # Save triples after processing each ontology\n",
    "    save_triples(onto_id, train_all, val_all, test_all)\n",
    "    print(f\"Finished processing ontology {onto_id}. Data saved.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
