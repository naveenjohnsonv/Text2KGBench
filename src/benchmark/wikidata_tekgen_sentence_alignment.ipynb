{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7204faf9-d1b1-475f-890b-519ac0051d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import http.client\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from SPARQLWrapper.SPARQLExceptions import QueryBadFormed, EndPointInternalError, EndPointNotFound, Unauthorized, URITooLong, SPARQLWrapperException\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c5b6466-2d46-4f27-b0ce-e14530a8cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql_query_cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d1d24a2-dc53-444b-8aeb-887afa3150f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_date_string(date_string):\n",
    "    pattern = r\"^(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})Z$\"\n",
    "    match = re.match(pattern, date_string)\n",
    "    if match:\n",
    "        year, month, day, hour, minute, second = match.groups()\n",
    "        date = datetime(int(year), int(month), int(day))\n",
    "        month_name = date.strftime(\"%B\")\n",
    "        new_date_string = f\"{day} {month_name} {year}\"\n",
    "        return new_date_string\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_splits(triples, splits = [0.4, 0.3, 0.3]):\n",
    "    triples = np.array(triples)\n",
    "    indices = np.random.permutation(triples.shape[0])\n",
    "    train_count = int(triples.shape[0] * splits[0])\n",
    "    val_count = int(triples.shape[0] * splits[1])\n",
    "    test_count = triples.shape[0] - train_count - val_count\n",
    "    train_triples = triples[indices[:train_count]]\n",
    "    val_triples = triples[indices[train_count:train_count+val_count]]\n",
    "    test_triples = triples[indices[train_count+val_count:]]\n",
    "    return train_triples.tolist(), val_triples.tolist(), test_triples.tolist()\n",
    "\n",
    "def save_triples(onto_id, train_all, val_all, test_all):\n",
    "    # Define base paths for different directories\n",
    "    base_paths = {\n",
    "        'train': \"../../data/wikidata_tekgen/train\",\n",
    "        'validation': \"../../data/wikidata_tekgen/validation\",\n",
    "        'ground_truth': \"../../data/wikidata_tekgen/ground_truth\",\n",
    "        'test': \"../../data/wikidata_tekgen/test\"\n",
    "    }\n",
    "    \n",
    "    # Ensure all required directories exist\n",
    "    for path in base_paths.values():\n",
    "        ensure_directory_exists(path)\n",
    "    \n",
    "    # Save train data\n",
    "    with open(f\"{base_paths['train']}/{onto_id}_train.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(train_all):\n",
    "            data = {\"id\": f\"{onto_id}_train_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2], \n",
    "                   \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "    \n",
    "    # Save validation data        \n",
    "    with open(f\"{base_paths['validation']}/{onto_id}_validation.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(val_all):\n",
    "            data = {\"id\": f\"{onto_id}_val_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2], \n",
    "                   \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    ''' # Not extracted correctly, eval code from original repository requires different format, ground truth in original repository also different format\n",
    "    # Save ground truth data        \n",
    "    with open(f\"{base_paths['ground_truth']}/{onto_id}_ground_truth.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(test_all):\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sub_label\": tr[0], \"rel_label\": tr[1], \"obj_label\": tr[2], \n",
    "                   \"sent\": tr[6], \"sub\": tr[3], \"rel\": tr[4], \"obj\": tr[5]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "    '''\n",
    "\n",
    "    # Save ground truth data        \n",
    "    with open(f\"{base_paths['ground_truth']}/{onto_id}_ground_truth.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(test_all):\n",
    "            triples = [{\"sub\": tr[0], \"rel\": tr[1], \"obj\": tr[2]}]\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": tr[6], \"triples\": triples}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "    # Save test data        \n",
    "    with open(f\"{base_paths['test']}/{onto_id}_test.jsonl\", \"w\") as out_file:\n",
    "        for idx, tr in enumerate(test_all):\n",
    "            data = {\"id\": f\"{onto_id}_test_{idx+1}\", \"sent\": tr[6]}\n",
    "            out_file.write(f\"{json.dumps(data)}\\n\")\n",
    "\n",
    "def get_triples_with_sentences(relation_pid: str, relation_label: str, rel_domain: str, rel_range: str,\n",
    "                               limit: int = 200, max_retries: int = 10):\n",
    "    assert relation_pid, \"relation id can't be empty\"\n",
    "    assert rel_domain, \"domain can't be empty\"\n",
    "\n",
    "    current_limit = 10000  # Start with a high limit for SPARQL query\n",
    "    retries = 0\n",
    "    # Set the User-Agent according to Wikidata's policy\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Build the SPARQL query\n",
    "            sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent=user_agent)\n",
    "            query = \"PREFIX wdt: <http://www.wikidata.org/prop/direct/> \\n PREFIX wd: <http://www.wikidata.org/entity/> \\n\"\n",
    "            query += \"SELECT DISTINCT ?sub ?subEntity ?objEntity ?objLabel { \\n ?subEntity wdt:P31/wdt:P279* wd:\" + rel_domain + \" . \\n\"\n",
    "            query += '?subEntity rdfs:label ?sub . FILTER (lang(?sub) = \"en\") \\n '\n",
    "            query += '?subEntity wdt:' + relation_pid + ' ?objEntity . \\n'\n",
    "            if rel_range and rel_range != \"\":\n",
    "                query += '?objEntity wdt:P31*/wdt:P279* wd:' + rel_range + ' . \\n '\n",
    "            query += 'OPTIONAL { ?objEntity rdfs:label ?objLabel . FILTER (lang(?objLabel) = \"en\") } \\n } '\n",
    "            # Set the dynamic LIMIT\n",
    "            query += f\"LIMIT {current_limit}\"\n",
    "            if show_query:\n",
    "                print(query)\n",
    "\n",
    "            if query in sparql_query_cache:\n",
    "                # Use cached results if available\n",
    "                triples = sparql_query_cache[query]\n",
    "            else:\n",
    "                # Execute the query and get a set of triples\n",
    "                triples = list()\n",
    "                subject_counter, object_counter = Counter(), Counter()\n",
    "                secondary_triples = list()\n",
    "                sparql.setQuery(query)\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                sparql.setTimeout(300)  # Set timeout to 5 minutes\n",
    "                sparql.setMethod('POST')\n",
    "\n",
    "                # Set the User-Agent\n",
    "                sparql.agent = user_agent\n",
    "\n",
    "                # Attempt to execute the query\n",
    "                response = sparql.query()\n",
    "                results = response.convert()\n",
    "\n",
    "                print(f'  {len(results[\"results\"][\"bindings\"])} SPARQL results.')\n",
    "                for result in results[\"results\"][\"bindings\"]:\n",
    "                    t_subject = result['sub']['value']\n",
    "                    if 'objLabel' in result:\n",
    "                        t_object = result['objLabel']['value']\n",
    "                        t_object_id = result['objEntity']['value'].replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                    else:\n",
    "                        t_object = result['objEntity']['value']\n",
    "                        date_string = convert_date_string(t_object)\n",
    "                        if date_string:\n",
    "                            t_object = date_string\n",
    "                        t_object_id = None\n",
    "                    t_subject_id = result['subEntity']['value'].replace(\"http://www.wikidata.org/entity/\", \"\")\n",
    "                    triple = [t_subject, relation_label, t_object, t_subject_id, relation_pid, t_object_id]\n",
    "                    # To get a diverse dataset, ignore subject/object if they occur more than 10% of the limit\n",
    "                    subject_counter[t_subject] += 1\n",
    "                    object_counter[t_object] += 1\n",
    "                    if subject_counter[t_subject] > (limit / 10) or object_counter[t_object] > (limit / 10):\n",
    "                        secondary_triples.append(triple)\n",
    "                        continue\n",
    "                    triples.append(triple)\n",
    "\n",
    "                # Append secondary triples\n",
    "                triples += secondary_triples\n",
    "                sparql_query_cache[query] = triples\n",
    "\n",
    "            print(f\"  collected {len(triples)} triples\")\n",
    "            if show_sample:\n",
    "                print(f\"  sample:\")\n",
    "                for tr in triples[:5]:\n",
    "                    print(f\"    {tr[:3]}\")\n",
    "\n",
    "            triples_with_sentences = list()\n",
    "            for tr in triples:\n",
    "                search_key = create_key(tr[0], tr[1], tr[2])\n",
    "                if search_key in sent_index:\n",
    "                    sentence = sent_index[search_key]\n",
    "                else:\n",
    "                    continue\n",
    "                tr.append(sentence)\n",
    "                triples_with_sentences.append(tr)\n",
    "\n",
    "                # Stop at the desired limit\n",
    "                if len(triples_with_sentences) >= limit:\n",
    "                    break\n",
    "\n",
    "            # If successful, return the collected triples with sentences\n",
    "            return triples_with_sentences\n",
    "\n",
    "        except SPARQLWrapperException as e:\n",
    "            retries += 1\n",
    "            code = None\n",
    "            reason = str(e)\n",
    "\n",
    "            # Try to parse the HTTP status code from the exception message\n",
    "            match = re.search(r'status code (\\d+)', reason, re.IGNORECASE)\n",
    "            if match:\n",
    "                code = int(match.group(1))\n",
    "\n",
    "            if code == 429:\n",
    "                # HTTP 429: Too Many Requests\n",
    "                retry_after = '60'  # Default wait time\n",
    "                wait_time = int(retry_after)\n",
    "                print(f\"HTTP 429 error encountered. Waiting for {wait_time} seconds before retrying ({retries}/{max_retries})...\")\n",
    "                time.sleep(wait_time)\n",
    "            elif code == 500:\n",
    "                print(f\"HTTP 500 error encountered on attempt {retries}/{max_retries}. Reducing LIMIT and retrying...\")\n",
    "                current_limit = max(10, current_limit // 2)\n",
    "                #time.sleep(5)\n",
    "            else:\n",
    "                print(f\"HTTP error {code} encountered: {reason}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "                time.sleep(5)\n",
    "        except (http.client.IncompleteRead, json.JSONDecodeError) as e:\n",
    "            retries += 1\n",
    "            print(f\"An error occurred: {e}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "            # Reduce the LIMIT and retry\n",
    "            current_limit = max(10, current_limit // 2)\n",
    "            print(f\"Reducing LIMIT to {current_limit} and retrying...\")\n",
    "            #time.sleep(5)\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"An error occurred: {e}. Retrying attempt {retries}/{max_retries} after short wait...\")\n",
    "            #time.sleep(5)\n",
    "\n",
    "    print(\"Max retries reached. Skipping this relation.\")\n",
    "    return []\n",
    "\n",
    "def create_key(sub_label, rel_label, obj_label):\n",
    "    # remove spaces and make lower case\n",
    "    sub_label = re.sub(r\"\\s+\", '', sub_label).lower()\n",
    "    rel_label = re.sub(r\"\\s+\", '', rel_label).lower()\n",
    "    obj_label = re.sub(r\"\\s+\", '', obj_label).lower()\n",
    "    # concatanate them \n",
    "    tr_key = f\"{sub_label}{rel_label}{obj_label}\"\n",
    "    return tr_key\n",
    "\n",
    "def ensure_directory_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1477d7d4-c1f3-4123-b2a4-cd28ba55a03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TekGen corpus processing started!\n",
      "\ttriple-to-sent index with 11358950 triples loaded in 2.20 mins!\n"
     ]
    }
   ],
   "source": [
    "# Load the TekGen corpus\n",
    "sent_index = dict()\n",
    "start_time = time.time()\n",
    "print(\"TekGen corpus processing started!\")\n",
    "with open('../../tekgen.csv') as csv_in_file:\n",
    "    sent_reader = csv.reader(csv_in_file)\n",
    "    next(sent_reader)\n",
    "    for row in sent_reader:\n",
    "        tr_key = create_key(row[0], row[1], row[2])\n",
    "        sent = row[4]\n",
    "        sent_index[tr_key] = sent\n",
    "        elapsed_time = (time.time()-start_time)/60\n",
    "    print(f\"\\ttriple-to-sent index with {len(sent_index)} triples loaded in {elapsed_time:.2f} mins!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c801336-be11-4395-83eb-d889b334289e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology: Culture Ontology (ont_10_culture)\n",
      "\n",
      "processing \"ethnic group\" (P172) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Mohd Noh Rajab', 'ethnic group', 'Malays']\n",
      "    ['Yusril Ihza Mahendra', 'ethnic group', 'Malays']\n",
      "    ['Lucas Fernandez de Piedrahita', 'ethnic group', 'Quechua people']\n",
      "    ['Eva Copa', 'ethnic group', 'Aymara']\n",
      "    ['Aru Apaza', 'ethnic group', 'Aymara']\n",
      "    70 triples with sentences in 0.08 seconds!\n",
      "\n",
      "processing \"religious order\" (P611) relation:\n",
      "  collected 0 triples\n",
      "  sample:\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"languages spoken, written or signed\" (P1412) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Chang Chih-Chia', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Ang Ui-jin', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Tsai Pei-huo', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Wang Chien-shien', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    ['Wu Chih-chung', 'languages spoken, written or signed', 'Standard Taiwanese Mandarin']\n",
      "    184 triples with sentences in 0.07 seconds!\n",
      "\n",
      "processing \"inception\" (P571) relation:\n",
      "  collected 334 triples\n",
      "  sample:\n",
      "    ['International Exhibition of Nothing', 'inception', '13 July 2015']\n",
      "    ['The Young God Ebisu (Waka ebisu)', 'inception', '01 January 1789']\n",
      "    ['Gifts from the Ebb Tide (Shiohi no tsuto)', 'inception', '01 January 1789']\n",
      "    ['Il giornalino della Domenica', 'inception', '01 January 1906']\n",
      "    ['The Discovery of America', 'inception', '01 January 1781']\n",
      "    3 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"start time\" (P580) relation:\n",
      "  collected 618 triples\n",
      "  sample:\n",
      "    ['Madrid International Film Festival', 'start time', '01 January 1997']\n",
      "    ['José Francisco Rosado Short Film Festival', 'start time', '01 January 1997']\n",
      "    ['Saix International Film Festival', 'start time', '01 January 2006']\n",
      "    ['Calgary Folk Music Festival 2023', 'start time', '27 July 2023']\n",
      "    ['Calgary Folk Music Festival 2024', 'start time', '25 July 2024']\n",
      "    14 triples with sentences in 0.01 seconds!\n",
      "\n",
      "processing \"dedicated to\" (P825) relation:\n",
      "  collected 17 triples\n",
      "  sample:\n",
      "    ['Agustín Rodríguez Sahagún', 'dedicated to', 'Agustín Rodríguez Sahagún']\n",
      "    ['Pianta di Roma (Pérac-Lafréry)', 'dedicated to', 'Henry III of France']\n",
      "    ['Essex, actually surveyed, with the several Roads from London...', 'dedicated to', 'Arthur Capell, 1st Earl of Essex']\n",
      "    ['2551913', 'dedicated to', 'Jean Jaurès']\n",
      "    ['Hommage à Arago', 'dedicated to', 'François Arago']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"iconographic symbol\" (P4185) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "  collected 39 triples\n",
      "  sample:\n",
      "    ['Philomena', 'iconographic symbol', \"martyr's palm\"]\n",
      "    ['Paul the Apostle', 'iconographic symbol', 'elephant ivory']\n",
      "    ['Paul the Apostle', 'iconographic symbol', 'sword']\n",
      "    ['Agatha of Sicily', 'iconographic symbol', \"martyr's palm\"]\n",
      "    ['Linus', 'iconographic symbol', 'religious habit']\n",
      "    2 triples with sentences in 483.27 seconds!\n",
      "\n",
      "processing \"indigenous to\" (P2341) relation:\n",
      "  collected 7 triples\n",
      "  sample:\n",
      "    ['Ntukpo Dance', 'indigenous to', 'Igbo people']\n",
      "    ['Odegelu Dance', 'indigenous to', 'Igbo people']\n",
      "    ['Dance de Yebra de Basa', 'indigenous to', 'Yebra de Basa']\n",
      "    ['Dance de San Lorenzo', 'indigenous to', 'Huesca']\n",
      "    ['Igede and Igba-Udugongo.', 'indigenous to', 'Igbo people']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "Ontology: Military Ontology (ont_5_military)\n",
      "\n",
      "processing \"military rank\" (P410) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Wilhelm Helten', 'military rank', 'Sturmscharführer']\n",
      "    ['Alexander Kolchak', 'military rank', 'admiral']\n",
      "    ['Zhou Yu', 'military rank', 'admiral']\n",
      "    ['Gaspard II de Coligny', 'military rank', 'admiral']\n",
      "    ['Franz Podezin', 'military rank', 'Sturmscharführer']\n",
      "    200 triples with sentences in 0.04 seconds!\n",
      "\n",
      "processing \"military branch\" (P241) relation:\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['George Orwell', 'military branch', 'International Brigades']\n",
      "    ['Marcel Doret', 'military branch', 'French Air Force']\n",
      "    ['Simón Radowitzky', 'military branch', 'International Brigades']\n",
      "    ['Erwin Rommel', 'military branch', 'Imperial German Army']\n",
      "    ['James Cook', 'military branch', 'Royal Navy']\n",
      "    200 triples with sentences in 0.02 seconds!\n",
      "\n",
      "processing \"military casualty classification \" (P1347) relation:\n",
      "  collected 6215 triples\n",
      "  sample:\n",
      "    ['Paul Bunker', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Pyotr Gavrilov', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Ernst Hartmann', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Hajo Rose', 'military casualty classification ', 'prisoner of war']\n",
      "    ['Fritz Fabricius', 'military casualty classification ', 'prisoner of war']\n",
      "    200 triples with sentences in 0.05 seconds!\n",
      "\n",
      "processing \"designed by\" (P287) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "  collected 100 triples\n",
      "  sample:\n",
      "    ['8.35 cm PL kanon vz. 22', 'designed by', 'Škoda Works']\n",
      "    ['Ehrhardt 7.5 cm Model 1904', 'designed by', 'Rheinmetall AG']\n",
      "    ['28 cm K L/40 \"Kurfürst\"', 'designed by', 'Krupp']\n",
      "    ['17 cm Kanone (E)', 'designed by', 'Krupp']\n",
      "    ['15 cm hrubá houfnice vz. 25', 'designed by', 'Škoda Works']\n",
      "    7 triples with sentences in 120.90 seconds!\n",
      "\n",
      "processing \"designed by\" (P287) relation:\n",
      "  collected 4 triples\n",
      "  sample:\n",
      "    ['Pindad Komodo', 'designed by', 'Pindad']\n",
      "    ['M3 Bradley', 'designed by', 'FMC Corporation']\n",
      "    ['ASLAV', 'designed by', 'Mowag']\n",
      "    ['Puma armored engineering vehicle', 'designed by', 'Israel Military Industries']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"commanded by\" (P4791) relation:\n",
      "  collected 337 triples\n",
      "  sample:\n",
      "    ['Marineamt', 'commanded by', 'Albrecht Obermaier']\n",
      "    ['Army Office', 'commanded by', 'Horst Wenner']\n",
      "    ['Naval Medical Institute', 'commanded by', 'Armin Wandel']\n",
      "    ['Marineamt', 'commanded by', 'Jürgen Geier']\n",
      "    ['Army Office', 'commanded by', 'Hellmuth Mäder']\n",
      "    0 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"next higher rank\" (P3730) relation:\n",
      "  collected 447 triples\n",
      "  sample:\n",
      "    ['Seekadett', 'next higher rank', 'http://www.wikidata.org/entity/Q131354754']\n",
      "    ['SS-Brigadeführer', 'next higher rank', 'SS-Gruppenführer']\n",
      "    ['Fähnrich', 'next higher rank', 'Oberfähnrich']\n",
      "    ['Sturmscharführer', 'next higher rank', 'Untersturmführer']\n",
      "    ['Oberst', 'next higher rank', 'generalmajor']\n",
      "    7 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"designated as terrorist by\" (P3461) relation:\n",
      "  collected 234 triples\n",
      "  sample:\n",
      "    ['Babbar Khalsa', 'designated as terrorist by', 'India']\n",
      "    ['Continuity Irish Republican Army', 'designated as terrorist by', 'New Zealand']\n",
      "    [\"People's Mojahedin Organization of Iran\", 'designated as terrorist by', 'Iran']\n",
      "    ['Harkat-ul-Mujahideen', 'designated as terrorist by', 'Bahrain']\n",
      "    ['Harkat-ul-Mujahideen', 'designated as terrorist by', 'India']\n",
      "    24 triples with sentences in 0.00 seconds!\n",
      "\n",
      "processing \"wing configuration\" (P1654) relation:\n",
      "  collected 117 triples\n",
      "  sample:\n",
      "    ['Heinkel HD 27', 'wing configuration', 'biplane']\n",
      "    ['Focke-Wulf A 7', 'wing configuration', 'mid-wing aircraft']\n",
      "    ['BOK-5', 'wing configuration', 'low wing']\n",
      "    ['Friedrichshafen FF.34', 'wing configuration', 'biplane']\n",
      "    ['Aichi AB-6', 'wing configuration', 'biplane']\n",
      "    4 triples with sentences in 0.00 seconds!\n",
      "Ontology: Book Ontology (ont_4_book)\n",
      "\n",
      "processing \"illustrator\" (P110) relation:\n",
      "An error occurred: Expecting ',' delimiter: line 81566 column 8 (char 2063482). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 90782 column 4 (char 2300946). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  2500 SPARQL results.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['House of Mystery', 'illustrator', 'Tony Akins']\n",
      "    ['Hellblazer', 'illustrator', 'John Ridgway']\n",
      "    ['Thorgal', 'illustrator', 'Fred Vignaux']\n",
      "    ['Rocket Girl', 'illustrator', 'Amy Reeder Hadley']\n",
      "    ['Ipurbeltz', 'illustrator', 'Dani Fano']\n",
      "    200 triples with sentences in 151.49 seconds!\n",
      "\n",
      "processing \"followed by\" (P156) relation:\n",
      "An error occurred: Expecting ',' delimiter: line 11099 column 2 (char 311183). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 12773 column 88 (char 352222). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: Invalid control character at: line 11133 column 85 (char 311267). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "An error occurred: IncompleteRead(0 bytes read). Retrying attempt 4/10 after short wait...\n",
      "Reducing LIMIT to 625 and retrying...\n",
      "  625 SPARQL results.\n",
      "  collected 625 triples\n",
      "  sample:\n",
      "    ['Evangelium Vitae', 'followed by', 'Ut Unum Sint']\n",
      "    ['Custodi di quella fede', 'followed by', 'http://www.wikidata.org/entity/Q345640']\n",
      "    ['Weimar Constitution', 'followed by', 'Constitution of the German Democratic Republic (1949)']\n",
      "    ['Graves de Communi Re', 'followed by', 'http://www.wikidata.org/entity/Q643261']\n",
      "    ['Laborem Exercens', 'followed by', 'Slavorum Apostoli']\n",
      "    27 triples with sentences in 291.26 seconds!\n",
      "\n",
      "processing \"publication date\" (P577) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Kyokuhoku Rhapsody', 'publication date', '01 December 2011']\n",
      "    ['Berkeley Physics Course', 'publication date', '01 January 1965']\n",
      "    ['Of birds and beasts', 'publication date', '01 May 1935']\n",
      "    ['Of birds and beasts', 'publication date', '01 January 1933']\n",
      "    ['Economics', 'publication date', '01 January 1948']\n",
      "    87 triples with sentences in 2.09 seconds!\n",
      "\n",
      "processing \"author\" (P50) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Tokyo Tower', 'author', 'http://www.wikidata.org/entity/Q447073']\n",
      "    ['Chinese short short story: Chinese language textbook for Japanese', 'author', 'Jing Liu']\n",
      "    ['The children of 200 years', 'author', 'Kenzaburō Ōe']\n",
      "    ['Four Atlases of Myology by Van Horne and Sagemolen', 'author', 'Isabelle Bonnard']\n",
      "    ['Violent Cases', 'author', 'Neil Gaiman']\n",
      "    200 triples with sentences in 42.35 seconds!\n",
      "\n",
      "processing \"publisher\" (P123) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['On Gender, Labor, and Inequality', 'publisher', 'University of Illinois Press']\n",
      "    ['Actes de Pierre de Dreux, duc de Bretagne (1213-1237)', 'publisher', 'Presses Universitaires de Rennes']\n",
      "    ['Rettssoga, Vol. I', 'publisher', 'Universitetsforlaget']\n",
      "    ['Heligoland, Past and Present', 'publisher', 'Lulu Press, Inc.']\n",
      "    ['Affective Materialities: Reorienting the Body in Modernist Literature', 'publisher', 'University Press of Florida']\n",
      "    150 triples with sentences in 13.47 seconds!\n",
      "\n",
      "processing \"characters\" (P674) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['X-Men #6', 'characters', 'Beast']\n",
      "    ['X-Men #15', 'characters', 'Beast']\n",
      "    ['X-Men #19', 'characters', 'Beast']\n",
      "    ['X-Men #31', 'characters', 'Iron Man']\n",
      "    ['X-Men #58', 'characters', 'Beast']\n",
      "    102 triples with sentences in 40.50 seconds!\n",
      "\n",
      "processing \"editor\" (P98) relation:\n",
      "  4074 SPARQL results.\n",
      "  collected 4074 triples\n",
      "  sample:\n",
      "    ['Journal of Statistical Physics', 'editor', 'Joel Lebowitz']\n",
      "    ['The Journal of Legislative Studies', 'editor', 'Philip Norton, Baron Norton of Louth']\n",
      "    ['Les Guêpes', 'editor', 'Jean-Baptiste Alphonse Karr']\n",
      "    [\"Bulletin of the British Ornithologists' Club\", 'editor', 'Percy Lowe']\n",
      "    ['Positif', 'editor', 'Michel Ciment']\n",
      "    200 triples with sentences in 32.82 seconds!\n",
      "\n",
      "processing \"place of publication\" (P291) relation:\n",
      "An error occurred: Invalid control character at: line 7322 column 90 (char 188290). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 17022 column 95 (char 433777). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: IncompleteRead(0 bytes read). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "An error occurred: Invalid control character at: line 11238 column 109 (char 286427). Retrying attempt 4/10 after short wait...\n",
      "Reducing LIMIT to 625 and retrying...\n",
      "  625 SPARQL results.\n",
      "  collected 625 triples\n",
      "  sample:\n",
      "    ['Max Liebermann: des Meisters Gemälde in 304 Abbildungen', 'place of publication', 'Stuttgart']\n",
      "    ['Die Ammerseebahn', 'place of publication', 'Stuttgart']\n",
      "    ['100 Jahre Eisenbahn Landshut – Vilsbiburg – Neumarkt-St. Veit', 'place of publication', 'Vilsbiburg']\n",
      "    ['Hebrew and Aramaic Lexicon of the Old Testament', 'place of publication', 'Cologne']\n",
      "    ['Die Hauptbahn München–Simbach und ihre Zweigbahnen', 'place of publication', 'Egglham']\n",
      "    3 triples with sentences in 293.62 seconds!\n",
      "\n",
      "processing \"narrative location\" (P840) relation:\n",
      "An error occurred: IncompleteRead(0 bytes read). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Expecting ',' delimiter: line 1644 column 2 (char 40956). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 9227 column 3 (char 229350). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "An error occurred: Expecting value: line 9213 column 17 (char 229342). Retrying attempt 4/10 after short wait...\n",
      "Reducing LIMIT to 625 and retrying...\n",
      "An error occurred: Invalid control character at: line 1636 column 89 (char 41027). Retrying attempt 5/10 after short wait...\n",
      "Reducing LIMIT to 312 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 1636 column 2 (char 40953). Retrying attempt 6/10 after short wait...\n",
      "Reducing LIMIT to 156 and retrying...\n",
      "  156 SPARQL results.\n",
      "  collected 156 triples\n",
      "  sample:\n",
      "    ['La Belle Province', 'narrative location', 'Contrecœur']\n",
      "    ['Maus', 'narrative location', 'United States of America']\n",
      "    ['Teenage Mutant Ninja Turtles Meet the Conservation Corps', 'narrative location', 'Amazon rainforest']\n",
      "    ['Maus', 'narrative location', 'Sosnowiec Ghetto']\n",
      "    ['Tintin in the Land of the Soviets', 'narrative location', 'Soviet Union']\n",
      "    5 triples with sentences in 389.96 seconds!\n",
      "\n",
      "processing \"genre\" (P136) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "  2500 SPARQL results.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Nothing Lasts Forever', 'genre', 'thriller novel']\n",
      "    ['Lover', 'genre', 'lesbian literature']\n",
      "    ['Thou Art the Man', 'genre', 'sensation fiction']\n",
      "    ['Breed of the Wolf', 'genre', 'thriller novel']\n",
      "    ['Laüstic', 'genre', 'court literature']\n",
      "    31 triples with sentences in 160.90 seconds!\n",
      "\n",
      "processing \"language of work or name\" (P407) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Grande Dizionario Enciclopedico UTET', 'language of work or name', 'Italian']\n",
      "    ['Batman Family', 'language of work or name', 'English']\n",
      "    ['Gnostic Apocalypse of Peter', 'language of work or name', 'Koine Greek']\n",
      "    ['Samurai: Heaven and Earth', 'language of work or name', 'English']\n",
      "    ['Serafino', 'language of work or name', 'Italian']\n",
      "    20 triples with sentences in 16.29 seconds!\n",
      "\n",
      "processing \"depicts\" (P180) relation:\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 166737 column 1 (char 4411975). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  5000 SPARQL results.\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    ['1Q84', 'depicts', 'NHK']\n",
      "    ['Abrahamic prayer', 'depicts', 'salah']\n",
      "    ['Abrahamic prayer', 'depicts', 'Sayyid']\n",
      "    ['Condorito', 'depicts', 'dog']\n",
      "    ['BE KOBE', 'depicts', 'K']\n",
      "    9 triples with sentences in 85.11 seconds!\n",
      "Ontology: Politics Ontology (ont_8_politics)\n",
      "\n",
      "processing \"head of state\" (P35) relation:\n",
      "  244 SPARQL results.\n",
      "  collected 244 triples\n",
      "  sample:\n",
      "    ['Andorra', 'head of state', 'Joan Enric Vives Sicília']\n",
      "    ['Croatia', 'head of state', 'Zoran Milanović']\n",
      "    ['Azerbaijan', 'head of state', 'Ilham Aliev']\n",
      "    ['Uzbekistan', 'head of state', 'Shavkat Mirziyoyev']\n",
      "    ['Liechtenstein', 'head of state', 'Hans-Adam II, Prince of Liechtenstein']\n",
      "    18 triples with sentences in 5.66 seconds!\n",
      "\n",
      "processing \"head of government\" (P6) relation:\n",
      "  244 SPARQL results.\n",
      "  collected 244 triples\n",
      "  sample:\n",
      "    ['Poland', 'head of government', 'Donald Tusk']\n",
      "    ['Tajikistan', 'head of government', 'Kokhir Rasulzoda']\n",
      "    ['Greece', 'head of government', 'Kyriakos Mitsotakis']\n",
      "    ['Dominican Republic', 'head of government', 'Luis Abinader']\n",
      "    ['Kenya', 'head of government', 'William Ruto']\n",
      "    4 triples with sentences in 2.59 seconds!\n",
      "\n",
      "processing \"position held\" (P39) relation:\n",
      "An error occurred: Invalid control character at: line 62487 column 107 (char 1587853). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 54735 column 87 (char 1391423). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  2500 SPARQL results.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Federico Errázuriz Echaurren', 'position held', 'President of Chile']\n",
      "    ['Georges Pompidou', 'position held', 'President of the French Republic']\n",
      "    ['Jacques Chirac', 'position held', 'President of the French Republic']\n",
      "    ['Jorge Montt', 'position held', 'President of Chile']\n",
      "    ['François Mitterrand', 'position held', 'President of the French Republic']\n",
      "    200 triples with sentences in 165.91 seconds!\n",
      "\n",
      "processing \"Member of\" (P463) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "  312 SPARQL results.\n",
      "  collected 312 triples\n",
      "  sample:\n",
      "    ['Carl Linnaeus', 'Member of', 'Royal Society']\n",
      "    ['Ludwig van Beethoven', 'Member of', 'Royal Netherlands Academy of Arts and Sciences']\n",
      "    ['Paul Morand', 'Member of', 'Académie Française']\n",
      "    ['Claude Bourgelat', 'Member of', 'French Academy of Sciences']\n",
      "    ['Louis Pasteur', 'Member of', 'Royal Society']\n",
      "    7 triples with sentences in 346.38 seconds!\n",
      "\n",
      "processing \"political alignment\" (P1387) relation:\n",
      "  3645 SPARQL results.\n",
      "  collected 3645 triples\n",
      "  sample:\n",
      "    ['Solidarity Civic Unity', 'political alignment', 'right-wing']\n",
      "    ['Croatian Growth', 'political alignment', 'right-wing']\n",
      "    ['The Independents', 'political alignment', 'right-wing']\n",
      "    ['League of Catalonia–Catalan Liberal Party', 'political alignment', 'right-wing']\n",
      "    ['Northern Ireland Unionist Party', 'political alignment', 'right-wing']\n",
      "    1 triples with sentences in 9.45 seconds!\n",
      "\n",
      "processing \"member of political party\" (P102) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Carlos Dávila', 'member of political party', 'Socialist Party of Chile']\n",
      "    ['Manuel L. Quezon', 'member of political party', 'Nacionalista Party']\n",
      "    ['Willi Stoph', 'member of political party', 'Socialist Unity Party of Germany']\n",
      "    ['Manuel Valls', 'member of political party', 'Socialist Party']\n",
      "    ['Ramón Freire', 'member of political party', 'Liberal Party']\n",
      "    200 triples with sentences in 6.83 seconds!\n",
      "\n",
      "processing \"elected in\" (P2715) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 9/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 10/10. Reducing LIMIT and retrying...\n",
      "Max retries reached. Skipping this relation.\n",
      "    0 triples with sentences in 602.16 seconds!\n",
      "\n",
      "processing \"candidacy in election\" (P3602) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Nicolas Sarkozy', 'candidacy in election', '2012 French presidential election']\n",
      "    ['Christian Wulff', 'candidacy in election', '2010 German presidential election']\n",
      "    ['Benigno Aquino III', 'candidacy in election', 'Philippine presidential election, 2010']\n",
      "    ['Donald Tusk', 'candidacy in election', '2005 Polish presidential election']\n",
      "    ['Stephen Harper', 'candidacy in election', '2011 Canadian federal election']\n",
      "    3 triples with sentences in 22.67 seconds!\n",
      "\n",
      "processing \"political ideology\" (P1142) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    [\"Ploughmen's Front\", 'political ideology', 'left-wing populism']\n",
      "    ['National Progressive Unionist Party', 'political ideology', 'left-wing populism']\n",
      "    ['Nationalist Civic Crusade', 'political ideology', 'http://www.wikidata.org/entity/Q6072327']\n",
      "    ['Belarusian Socialist-Revolutionary Party', 'political ideology', 'left-wing populism']\n",
      "    ['Herritar Batasuna', 'political ideology', 'anarcho-communism']\n",
      "    138 triples with sentences in 4.15 seconds!\n",
      "Ontology: Computer Ontology (ont_6_computer)\n",
      "\n",
      "processing \"developer\" (P178) relation:\n",
      "An error occurred: Invalid control character at: line 75767 column 86 (char 1884087). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 85059 column 123 (char 2113458). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  2500 SPARQL results.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Netscape Navigator 4.05', 'developer', 'Netscape']\n",
      "    ['Internet Channel', 'developer', 'Opera Software']\n",
      "    ['SOAtest', 'developer', 'Parasoft']\n",
      "    ['360 Secure Browser', 'developer', 'Qihoo 360']\n",
      "    ['JBuilder', 'developer', 'CodeGear']\n",
      "    200 triples with sentences in 154.73 seconds!\n",
      "\n",
      "processing \"creator\" (P170) relation:\n",
      "  371 SPARQL results.\n",
      "  collected 371 triples\n",
      "  sample:\n",
      "    ['csv2ledger', 'creator', 'Joost Kremers']\n",
      "    ['openstreetmap-website', 'creator', 'Steve Coast']\n",
      "    ['WEB', 'creator', 'Donald Knuth']\n",
      "    ['TeX', 'creator', 'Donald Knuth']\n",
      "    ['GNU Hyperbole', 'creator', 'Robert Weiner']\n",
      "    22 triples with sentences in 6.35 seconds!\n",
      "\n",
      "processing \"platform\" (P400) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['iOS 7', 'platform', 'iPad Air']\n",
      "    ['Face ID', 'platform', 'iPhone 14']\n",
      "    ['TVEDIT', 'platform', 'PDP-1']\n",
      "    ['CEMM', 'platform', 'IBM PC compatible']\n",
      "    ['VisiCalc', 'platform', 'Apple II']\n",
      "    200 triples with sentences in 27.04 seconds!\n",
      "\n",
      "processing \"operating system\" (P306) relation:\n",
      "  1868 SPARQL results.\n",
      "  collected 1868 triples\n",
      "  sample:\n",
      "    ['FM Towns', 'operating system', 'Gentoo Linux']\n",
      "    ['Nokia 5230', 'operating system', 'Symbian']\n",
      "    ['Nokia 5530 XpressMusic', 'operating system', 'Symbian']\n",
      "    ['Nokia E71', 'operating system', 'Symbian']\n",
      "    ['Nokia 6630', 'operating system', 'Symbian']\n",
      "    53 triples with sentences in 2.17 seconds!\n",
      "Ontology: Movie Ontology (ont_1_movie)\n",
      "\n",
      "processing \"director\" (P57) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Help Wanted', 'director', 'Stephen Hillenburg']\n",
      "    ['Green Vs. Red', 'director', 'Shigeyuki Miya']\n",
      "    ['My Neighbors the Yamadas', 'director', 'Isao Takahata']\n",
      "    ['Pilot', 'director', 'Michael Fresco']\n",
      "    ['Project A-ko 2: Plot of the Daitokuji Financial Group', 'director', 'Yuji Moriyama']\n",
      "    200 triples with sentences in 18.90 seconds!\n",
      "\n",
      "processing \"screenwriter\" (P58) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['From Up on Poppy Hill', 'screenwriter', 'Hayao Miyazaki']\n",
      "    ['Dragon Ball: Yo! Son Goku and His Friends Return!!', 'screenwriter', 'Takao Koyama']\n",
      "    ['The Sky Crawlers', 'screenwriter', 'Hiroshi Mori']\n",
      "    ['The Muppets Valentine Show', 'screenwriter', 'Jerry Juhl']\n",
      "    ['Locke the Superman', 'screenwriter', 'Atsushi Yamatoya']\n",
      "    200 triples with sentences in 34.84 seconds!\n",
      "\n",
      "processing \"genre\" (P136) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['The Rescuers', 'genre', 'film based on literature']\n",
      "    ['The Little Fox', 'genre', 'film based on literature']\n",
      "    ['Tatarak', 'genre', 'film based on literature']\n",
      "    ['Mutiny on the Bounty', 'genre', 'film based on literature']\n",
      "    ['Metropolis', 'genre', 'film based on literature']\n",
      "    6 triples with sentences in 7.56 seconds!\n",
      "\n",
      "processing \"based on\" (P144) relation:\n",
      "An error occurred: Invalid control character at: line 48889 column 94 (char 1236767). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Invalid control character at: line 48908 column 122 (char 1236766). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 31947 column 8 (char 810761). Retrying attempt 3/10 after short wait...\n",
      "Reducing LIMIT to 1250 and retrying...\n",
      "  1250 SPARQL results.\n",
      "  collected 1250 triples\n",
      "  sample:\n",
      "    ['La Dame de Monsoreau', 'based on', 'La Dame de Monsoreau']\n",
      "    ['Anne of Green Gables', 'based on', 'Anne of Green Gables']\n",
      "    ['Harlock: Space Pirate', 'based on', 'Captain Harlock']\n",
      "    ['Patlabor: The New Files', 'based on', 'Patlabor']\n",
      "    [\"Kino's Journey: Life Goes On\", 'based on', \"Kino's Journey\"]\n",
      "    53 triples with sentences in 211.48 seconds!\n",
      "\n",
      "processing \"cast member\" (P161) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Aquaman', 'cast member', 'Ving Rhames']\n",
      "    ['Football Wives', 'cast member', 'Ving Rhames']\n",
      "    ['Pilot', 'cast member', 'Martin Sheen']\n",
      "    [\"Everybody's Favorite Bagman\", 'cast member', 'Steven Hill']\n",
      "    ['Pilot', 'cast member', 'Dominic Monaghan']\n",
      "    200 triples with sentences in 39.39 seconds!\n",
      "\n",
      "processing \"award received\" (P166) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Star Wars: Episode IV – A New Hope', 'award received', 'Academy Award for Best Film Editing']\n",
      "    ['American Beauty', 'award received', 'BAFTA Award for Best Film']\n",
      "    ['Nights of Cabiria', 'award received', 'Academy Award for Best International Feature Film']\n",
      "    ['Star Wars: Episode IV – A New Hope', 'award received', 'Academy Award for Best Production Design']\n",
      "    ['The Hunt', 'award received', 'Nordic Council Film Prize']\n",
      "    200 triples with sentences in 38.51 seconds!\n",
      "\n",
      "processing \"production company\" (P272) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['The Lonely Trail', 'production company', 'Republic Pictures']\n",
      "    ['Down Laredo Way', 'production company', 'Republic Pictures']\n",
      "    ['Iron Mountain Trail', 'production company', 'Republic Pictures']\n",
      "    ['The Crimson Ghost', 'production company', 'Republic Pictures']\n",
      "    ['Old Overland Trail', 'production company', 'Republic Pictures']\n",
      "    200 triples with sentences in 4.42 seconds!\n",
      "\n",
      "processing \"country of origin\" (P495) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Se le movió el piso: A portrait of Managua', 'country of origin', 'Nicaragua']\n",
      "    ['The Light Thief', 'country of origin', 'Kyrgyzstan']\n",
      "    ['Sandino', 'country of origin', 'Nicaragua']\n",
      "    ['The Chimp', 'country of origin', 'Kyrgyzstan']\n",
      "    ['The Ghost of War', 'country of origin', 'Nicaragua']\n",
      "    200 triples with sentences in 5.15 seconds!\n",
      "\n",
      "processing \"publication date\" (P577) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Sol Bianca', 'publication date', '01 January 1990']\n",
      "    ['Castle Fantasia: Seima Taisen', 'publication date', '01 January 1998']\n",
      "    ['Puni Puni ☆ Poemy', 'publication date', '01 January 2001']\n",
      "    ['One Piece: Defeat The Pirate Ganzak!', 'publication date', '26 July 1998']\n",
      "    ['Zaion: I Wish You Were Here', 'publication date', '01 January 2001']\n",
      "    200 triples with sentences in 2.20 seconds!\n",
      "\n",
      "processing \"characters\" (P674) relation:\n",
      "  3960 SPARQL results.\n",
      "  collected 3960 triples\n",
      "  sample:\n",
      "    ['The Musketeer', 'characters', 'Constance Bonacieux']\n",
      "    ['The Dark Knight', 'characters', 'Rachel Dawes']\n",
      "    ['Forrest Gump', 'characters', 'Forrest Gump']\n",
      "    ['The Dark Knight', 'characters', 'Gillian B. Loeb']\n",
      "    ['Teenage Mutant Ninja Turtles', 'characters', 'Hamato Yoshi']\n",
      "    200 triples with sentences in 5.25 seconds!\n",
      "\n",
      "processing \"narrative location\" (P840) relation:\n",
      "An error occurred: Invalid control character at: line 176523 column 128 (char 4316845). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  5000 SPARQL results.\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    ['Ride Your Wave', 'narrative location', 'Chiba']\n",
      "    ['Her Blue Sky', 'narrative location', 'Chichibu']\n",
      "    ['The Unmentionables', 'narrative location', 'Chicago']\n",
      "    ['Crayon Shin-chan: Fierceness That Invites Storm! Yakiniku Road of Honor', 'narrative location', 'Atami']\n",
      "    ['The Man with the Iron Heart', 'narrative location', 'Prague']\n",
      "    200 triples with sentences in 78.27 seconds!\n",
      "\n",
      "processing \"filming location\" (P915) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Make It Count', 'filming location', 'Singapore']\n",
      "    ['Victim', 'filming location', 'Mount Pearl']\n",
      "    ['A Gang Story', 'filming location', 'Villeurbanne']\n",
      "    ['Unutamam', 'filming location', 'Istanbul']\n",
      "    ['Diamante', 'filming location', 'Modena']\n",
      "    200 triples with sentences in 35.24 seconds!\n",
      "\n",
      "processing \"main subject\" (P921) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Strawberry Marshmallow', 'main subject', 'elementary school student']\n",
      "    [\"Howl's Moving Castle\", 'main subject', 'youth and old age']\n",
      "    ['Yōtōden', 'main subject', 'Japanese mythology']\n",
      "    [\"Howl's Moving Castle\", 'main subject', 'war']\n",
      "    ['Pilot', 'main subject', 'probability theory']\n",
      "    200 triples with sentences in 12.05 seconds!\n",
      "\n",
      "processing \"nominated for\" (P1411) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['A Beautiful Mind', 'nominated for', 'Academy Award for Best Makeup and Hairstyling']\n",
      "    ['When Marnie Was There', 'nominated for', 'Academy Award for Best Animated Feature']\n",
      "    ['Star Trek: The Motion Picture', 'nominated for', 'Academy Award for Best Visual Effects']\n",
      "    ['Star Trek', 'nominated for', 'Academy Award for Best Visual Effects']\n",
      "    ['The Wind Rises', 'nominated for', 'Academy Award for Best Animated Feature']\n",
      "    195 triples with sentences in 15.07 seconds!\n",
      "\n",
      "processing \"cost\" (P2130) relation:\n",
      "  6370 SPARQL results.\n",
      "  collected 6370 triples\n",
      "  sample:\n",
      "    ['Batgirl', 'cost', '90000000']\n",
      "    ['Chasing Amy', 'cost', '250000']\n",
      "    ['Tokyo Godfathers', 'cost', '2400000']\n",
      "    ['Scoob!', 'cost', '90000000']\n",
      "    ['My Neighbors the Yamadas', 'cost', '2000000000']\n",
      "    0 triples with sentences in 7.57 seconds!\n",
      "Ontology: Sports Ontology (ont_3_sport)\n",
      "\n",
      "processing \"occupation\" (P106) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Alou Diarra', 'occupation', 'association football player']\n",
      "    ['Jérémy Toulalan', 'occupation', 'association football player']\n",
      "    ['Karim Benzema', 'occupation', 'association football player']\n",
      "    ['Mathieu Valbuena', 'occupation', 'association football player']\n",
      "    ['Patrice Evra', 'occupation', 'association football player']\n",
      "    200 triples with sentences in 32.55 seconds!\n",
      "\n",
      "processing \"sport\" (P641) relation:\n",
      "An error occurred: Invalid control character at: line 51256 column 115 (char 1301598). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  5000 SPARQL results.\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    ['2008 Grand Slam of Darts', 'sport', 'darts']\n",
      "    ['2005 World Matchplay', 'sport', 'darts']\n",
      "    ['Las Vegas Desert Classic', 'sport', 'darts']\n",
      "    ['Dutch Open', 'sport', 'darts']\n",
      "    ['2011 New Zealand Winter Games', 'sport', 'Freeskiing']\n",
      "    200 triples with sentences in 114.81 seconds!\n",
      "\n",
      "processing \"member of sports team\" (P54) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Franck Ribéry', 'member of sports team', 'U.S. Salernitana 1919']\n",
      "    ['Zinedine Zidane', 'member of sports team', 'FC Girondins de Bordeaux']\n",
      "    ['Zinedine Zidane', 'member of sports team', 'Juventus FC']\n",
      "    ['Patrice Evra', 'member of sports team', 'Real Madrid CF']\n",
      "    ['Alessandro Del Piero', 'member of sports team', 'Odisha FC']\n",
      "    200 triples with sentences in 5.65 seconds!\n",
      "\n",
      "processing \"country for sport\" (P1532) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Marcus Tulio Tanaka', 'country for sport', 'Japan']\n",
      "    ['Koji Murofushi', 'country for sport', 'Japan']\n",
      "    ['Praewa Misato Philaphandeth', 'country for sport', 'Laos']\n",
      "    ['Charles Hamelin', 'country for sport', 'Canada']\n",
      "    ['Syl Apps', 'country for sport', 'Canada']\n",
      "    200 triples with sentences in 5.04 seconds!\n",
      "\n",
      "processing \"sports season of league or competition\" (P3450) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['2008–09 Fußball-Bundesliga', 'sports season of league or competition', 'Bundesliga']\n",
      "    ['2011–12 Fußball-Bundesliga', 'sports season of league or competition', 'Bundesliga']\n",
      "    ['2010–11 2. Fußball-Bundesliga', 'sports season of league or competition', '2. Frauen-Bundesliga']\n",
      "    ['1972-73 Cypriot Cup', 'sports season of league or competition', 'Cypriot Cup']\n",
      "    ['1986–87 Cypriot Cup', 'sports season of league or competition', 'Cypriot Cup']\n",
      "    112 triples with sentences in 15.19 seconds!\n",
      "\n",
      "processing \"coach of sports team\" (P6087) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Diego Maradona', 'coach of sports team', 'Racing Club de Avellaneda']\n",
      "    ['Tite', 'coach of sports team', 'Sociedade Esportiva Palmeiras']\n",
      "    ['Tite', 'coach of sports team', 'Al-Wahda S.C.C.']\n",
      "    ['Tite', 'coach of sports team', 'Ypiranga Futebol Clube']\n",
      "    ['Piet Kraak', 'coach of sports team', 'Velox']\n",
      "    57 triples with sentences in 60.56 seconds!\n",
      "\n",
      "processing \"league\" (P118) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Ben Foster', 'league', 'Premier League']\n",
      "    ['Gianfranco Zola', 'league', 'Premier League']\n",
      "    ['Manuel Almunia', 'league', 'Premier League']\n",
      "    ['Marlon King', 'league', 'Premier League']\n",
      "    ['Michael Owen', 'league', 'Premier League']\n",
      "    200 triples with sentences in 9.31 seconds!\n",
      "\n",
      "processing \"home venue\" (P159) relation:\n",
      "  71 SPARQL results.\n",
      "  collected 71 triples\n",
      "  sample:\n",
      "    ['FC BFV', 'home venue', 'Mahamasina Municipal Stadium']\n",
      "    ['Football Club Côtière Luenaz', 'home venue', 'Stade des Gravelles']\n",
      "    ['Club Vasconia', 'home venue', \"Pavelló de la Vall d'Hebron\"]\n",
      "    ['Stade bordelais', 'home venue', 'Stade Sainte-Germain']\n",
      "    ['Red Star F.C.', 'home venue', 'Stade de Paris']\n",
      "    3 triples with sentences in 15.75 seconds!\n",
      "\n",
      "processing \"country of origin \" (P495) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "  39 SPARQL results.\n",
      "  collected 39 triples\n",
      "  sample:\n",
      "    ['Chinlone', 'country of origin ', 'Myanmar']\n",
      "    ['Yaniv', 'country of origin ', 'Nepal']\n",
      "    ['3-2-5', 'country of origin ', 'Pakistan']\n",
      "    ['Lethwei', 'country of origin ', 'Myanmar']\n",
      "    ['Kok-borou', 'country of origin ', 'Kyrgyzstan']\n",
      "    3 triples with sentences in 525.62 seconds!\n",
      "\n",
      "processing \"league\" (P118) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Joe Walters', 'league', 'Major League Lacrosse']\n",
      "    ['Hassan Ali Akbari', 'league', 'Iranian Basketball Super League']\n",
      "    ['Drew Adams', 'league', 'Major League Lacrosse']\n",
      "    ['Quack Davis', 'league', 'Negro league baseball']\n",
      "    ['Mike Manley', 'league', 'Major League Lacrosse']\n",
      "    7 triples with sentences in 5.60 seconds!\n",
      "\n",
      "processing \"competition class\" (P2094) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    [\"Andorra women's national football team\", 'competition class', \"women's association football\"]\n",
      "    [\"Niger women's national football team\", 'competition class', \"women's association football\"]\n",
      "    [\"Gabon women's national football team\", 'competition class', \"women's association football\"]\n",
      "    ['Astra Hungary FC', 'competition class', \"women's association football\"]\n",
      "    [\"Anguilla women's national football team\", 'competition class', \"women's association football\"]\n",
      "    150 triples with sentences in 5.19 seconds!\n",
      "Ontology: Space Ontology (ont_7_space)\n",
      "\n",
      "processing \"site of astronomical discovery\" (P65) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['(7609) 1995 WX3', 'site of astronomical discovery', 'Nachi-Katsuura Observatory']\n",
      "    ['7533 Seiraiji', 'site of astronomical discovery', 'Nachi-Katsuura Observatory']\n",
      "    ['(8263) 1986 QT', 'site of astronomical discovery', 'La Silla Observatory']\n",
      "    ['6834 Hunfeld', 'site of astronomical discovery', 'Nachi-Katsuura Observatory']\n",
      "    ['(7534) 1995 UA7', 'site of astronomical discovery', 'Nachi-Katsuura Observatory']\n",
      "    35 triples with sentences in 8.11 seconds!\n",
      "\n",
      "processing \"minor planet group\" (P196) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 6/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 7/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 8/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 9/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 10/10. Reducing LIMIT and retrying...\n",
      "Max retries reached. Skipping this relation.\n",
      "    0 triples with sentences in 603.21 seconds!\n",
      "\n",
      "processing \"constellation\" (P59) relation:\n",
      "  1772 SPARQL results.\n",
      "  collected 1772 triples\n",
      "  sample:\n",
      "    ['NGC 5639', 'constellation', 'Boötes']\n",
      "    ['NGC 5888', 'constellation', 'Boötes']\n",
      "    ['NGC 3144', 'constellation', 'Draco']\n",
      "    ['NGC 4108', 'constellation', 'Draco']\n",
      "    ['NGC 4238', 'constellation', 'Draco']\n",
      "    126 triples with sentences in 3.79 seconds!\n",
      "\n",
      "processing \"astronaut mission\" (P450) relation:\n",
      "  1798 SPARQL results.\n",
      "  collected 1798 triples\n",
      "  sample:\n",
      "    ['Franklin Chang-Díaz', 'astronaut mission', 'STS-60']\n",
      "    ['Jeffrey Williams', 'astronaut mission', 'Soyuz TMA-16']\n",
      "    ['Akihiko Hoshide', 'astronaut mission', 'SpaceX Crew-2']\n",
      "    ['Viktor Gorbatko', 'astronaut mission', 'Soyuz 37']\n",
      "    ['Charles Camarda', 'astronaut mission', 'STS-114']\n",
      "    200 triples with sentences in 2.31 seconds!\n",
      "\n",
      "processing \"spacecraft docking/undocking date\" (P622) relation:\n",
      "  63 SPARQL results.\n",
      "  collected 63 triples\n",
      "  sample:\n",
      "    ['Soyuz TMA-18M', 'spacecraft docking/undocking date', '04 September 2015']\n",
      "    ['Cygnus NG-11', 'spacecraft docking/undocking date', '19 April 2019']\n",
      "    ['Cygnus NG-10', 'spacecraft docking/undocking date', '19 November 2018']\n",
      "    ['Soyuz TMA-18M', 'spacecraft docking/undocking date', '02 March 2016']\n",
      "    ['Cygnus NG-11', 'spacecraft docking/undocking date', '06 August 2019']\n",
      "    15 triples with sentences in 1.15 seconds!\n",
      "\n",
      "processing \"backup or reserve team or crew\" (P3015) relation:\n",
      "  236 SPARQL results.\n",
      "  collected 236 triples\n",
      "  sample:\n",
      "    ['Soyuz T-14', 'backup or reserve team or crew', 'Aleksandr Viktorenko']\n",
      "    ['Soyuz T-14', 'backup or reserve team or crew', 'Eugeni Vladimirovich Saley']\n",
      "    ['Gemini 6A', 'backup or reserve team or crew', 'John Young']\n",
      "    ['Apollo 8', 'backup or reserve team or crew', 'Fred Haise']\n",
      "    ['Soyuz 39', 'backup or reserve team or crew', 'Maidarzhavyn Ganzorig']\n",
      "    2 triples with sentences in 4.19 seconds!\n",
      "\n",
      "processing \"location of landing\" (P1158) relation:\n",
      "  188 SPARQL results.\n",
      "  collected 188 triples\n",
      "  sample:\n",
      "    ['Columbia', 'location of landing', 'North Pacific Ocean']\n",
      "    ['Apollo 14', 'location of landing', 'Pacific Ocean']\n",
      "    ['Apollo 16', 'location of landing', 'Pacific Ocean']\n",
      "    ['Low-Earth Orbit Flight Test of an Inflatable Decelerator', 'location of landing', 'Pacific Ocean']\n",
      "    ['Soyuz TM-4', 'location of landing', 'Kazakhstan']\n",
      "    22 triples with sentences in 47.51 seconds!\n",
      "Ontology: Nature Ontology (ont_9_nature)\n",
      "\n",
      "processing \"parent taxon\" (P171) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Banisterobates', 'parent taxon', 'Dinosauromorpha']\n",
      "    ['Beaconella', 'parent taxon', 'insect']\n",
      "    ['Agrestipus', 'parent taxon', 'Reptilia']\n",
      "    ['Tamias sibiricus', 'parent taxon', 'Eutamias']\n",
      "    ['Antipus', 'parent taxon', 'Crocodilia']\n",
      "    200 triples with sentences in 12.12 seconds!\n",
      "\n",
      "processing \"mountain range\" (P4552) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Rotfluh', 'mountain range', 'Silvretta Alps']\n",
      "    ['Spielberg', 'mountain range', 'Bohemian Forest']\n",
      "    ['Reischlberg', 'mountain range', 'Bohemian Forest']\n",
      "    ['Haagspitze', 'mountain range', 'Silvretta Alps']\n",
      "    ['Piz Jeramias', 'mountain range', 'Silvretta Alps']\n",
      "    200 triples with sentences in 20.33 seconds!\n",
      "\n",
      "processing \"parent peak\" (P3137) relation:\n",
      "  2710 SPARQL results.\n",
      "  collected 2710 triples\n",
      "  sample:\n",
      "    ['Matterhorn', 'parent peak', 'Weisshorn']\n",
      "    ['Ill Crag', 'parent peak', 'Scafell Pike']\n",
      "    ['Barrhorn', 'parent peak', 'Weisshorn']\n",
      "    ['Zugspitze', 'parent peak', 'Finsteraarhorn']\n",
      "    ['Crinkle Crags', 'parent peak', 'Scafell Pike']\n",
      "    200 triples with sentences in 10.90 seconds!\n",
      "\n",
      "processing \"taxon common name\" (P1843) relation:\n",
      "  250 SPARQL results.\n",
      "  collected 250 triples\n",
      "  sample:\n",
      "    ['Panthera uncia', 'taxon common name', 'snežni leopard']\n",
      "    ['Panthera uncia', 'taxon common name', '雪豹']\n",
      "    ['Chinese sturgeon', 'taxon common name', 'jeseter čínský']\n",
      "    ['Chinese sturgeon', 'taxon common name', 'Chinesischer Stör']\n",
      "    ['Chinese sturgeon', 'taxon common name', 'Chinese sturgeon']\n",
      "    0 triples with sentences in 0.68 seconds!\n",
      "\n",
      "processing \"tributary\" (P974) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Kibbutzim Stream', 'tributary', 'Naẖal Shoqeq']\n",
      "    ['Río de la Plata', 'tributary', 'Luján River']\n",
      "    ['Naẖal H̱illazon', 'tributary', 'Naẖal H̱anna']\n",
      "    ['Nahal Taninim', 'tributary', 'Naẖal Allona']\n",
      "    ['Nahal Dishon', 'tributary', 'Nahal Hatsor']\n",
      "    106 triples with sentences in 11.22 seconds!\n",
      "\n",
      "processing \"origin of the watercourse\" (P885) relation:\n",
      "An error occurred: Invalid control character at: line 62143 column 97 (char 1538439). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "An error occurred: Expecting property name enclosed in double quotes: line 52678 column 3 (char 1301288). Retrying attempt 2/10 after short wait...\n",
      "Reducing LIMIT to 2500 and retrying...\n",
      "  2500 SPARQL results.\n",
      "  collected 2500 triples\n",
      "  sample:\n",
      "    ['Koksoak River', 'origin of the watercourse', 'confluence']\n",
      "    ['Harricana River', 'origin of the watercourse', 'Lac Blouin']\n",
      "    ['Rönne River', 'origin of the watercourse', 'Ringsjön']\n",
      "    ['Lahn', 'origin of the watercourse', 'Lahn spring']\n",
      "    ['Amu Darya', 'origin of the watercourse', 'Pamir River']\n",
      "    119 triples with sentences in 161.11 seconds!\n",
      "\n",
      "processing \"mouth of the watercourse\" (P403) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Naẖal Kesalon', 'mouth of the watercourse', 'Nahal Sorek']\n",
      "    ['Kolva', 'mouth of the watercourse', 'Vishera']\n",
      "    ['Naẖal Salil', 'mouth of the watercourse', 'Nahal Tavor']\n",
      "    ['Nahal Ramot', 'mouth of the watercourse', 'Nahal Sorek']\n",
      "    ['Naẖal Nurit', 'mouth of the watercourse', 'Harod stream']\n",
      "    200 triples with sentences in 5.02 seconds!\n",
      "\n",
      "processing \"IUCN conservation status\" (P141) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Black Redstart', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Arvicola amphibius', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Blue-capped Rock Thrush', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Common Nightingale', 'IUCN conservation status', 'Least Concern']\n",
      "    ['Common Pheasant', 'IUCN conservation status', 'Least Concern']\n",
      "    0 triples with sentences in 4.17 seconds!\n",
      "\n",
      "processing \"taxon synonym\" (P1420) relation:\n",
      "  4284 SPARQL results.\n",
      "  collected 4284 triples\n",
      "  sample:\n",
      "    ['Arabian carpetshark', 'taxon synonym', 'Chiloscyllium confusum']\n",
      "    ['Bassia laniflora', 'taxon synonym', 'Kochia laniflora']\n",
      "    ['Notorynchus', 'taxon synonym', 'Notorhynchus']\n",
      "    ['Passifloraceae', 'taxon synonym', 'Malesherbiaceae']\n",
      "    ['Neophrissospongia tubulata', 'taxon synonym', 'Corallistes tubulatus']\n",
      "    93 triples with sentences in 9.27 seconds!\n",
      "\n",
      "processing \"reservoir created\" (P4661) relation:\n",
      "  3084 SPARQL results.\n",
      "  collected 3084 triples\n",
      "  sample:\n",
      "    ['Furnas Dam', 'reservoir created', 'Furnas Reservoir']\n",
      "    ['Cooby Dam', 'reservoir created', 'Cooby Creek Reservoir']\n",
      "    ['Vihar Dam', 'reservoir created', 'Vihar Lake']\n",
      "    ['Horse Mesa Dam', 'reservoir created', 'Apache Lake']\n",
      "    ['New Croton Dam', 'reservoir created', 'New Croton reservoir']\n",
      "    200 triples with sentences in 12.53 seconds!\n",
      "\n",
      "processing \"drainage basin\" (P4614) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Arros', 'drainage basin', 'Adour basin']\n",
      "    ['Ruisseau Veilleux', 'drainage basin', 'Restigouche River basin']\n",
      "    ['Río Manso', 'drainage basin', 'Puelo Basin']\n",
      "    ['Bullilleo River', 'drainage basin', 'Maule basin']\n",
      "    ['Rivière Assemetquagan', 'drainage basin', 'Restigouche River basin']\n",
      "    43 triples with sentences in 2.60 seconds!\n",
      "\n",
      "processing \"mountains classification \" (P4320) relation:\n",
      "  8814 SPARQL results.\n",
      "  collected 8814 triples\n",
      "  sample:\n",
      "    ['2003 Tour de Suisse', 'mountains classification ', 'Giuseppe Guerini']\n",
      "    ['2005 Tour de Suisse', 'mountains classification ', 'Chris Horner']\n",
      "    ['2006 Tour de Suisse', 'mountains classification ', 'Marco Fertonani']\n",
      "    ['2006 Tour de Suisse', 'mountains classification ', 'Martin Elmiger']\n",
      "    ['2004 Tour de Suisse', 'mountains classification ', 'Robert Hunter']\n",
      "    131 triples with sentences in 45.59 seconds!\n",
      "\n",
      "processing \"Habitat\" (P2974) relation:\n",
      "HTTP 500 error encountered on attempt 1/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 2/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 3/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 4/10. Reducing LIMIT and retrying...\n",
      "HTTP 500 error encountered on attempt 5/10. Reducing LIMIT and retrying...\n",
      "  88 SPARQL results.\n",
      "  collected 88 triples\n",
      "  sample:\n",
      "    ['Javan Pond Heron', 'Habitat', 'wetland']\n",
      "    ['Calluna vulgaris', 'Habitat', 'bog']\n",
      "    ['Arundinax aedon', 'Habitat', 'wetland']\n",
      "    ['chum salmon', 'Habitat', 'North Pacific Ocean']\n",
      "    ['Pink-breasted Flowerpecker', 'Habitat', 'wetland']\n",
      "    1 triples with sentences in 339.89 seconds!\n",
      "Ontology: Music Ontology (ont_2_music)\n",
      "\n",
      "processing \"composer\" (P86) relation:\n",
      "An error occurred: Invalid control character at: line 155529 column 96 (char 3865004). Retrying attempt 1/10 after short wait...\n",
      "Reducing LIMIT to 5000 and retrying...\n",
      "  5000 SPARQL results.\n",
      "  collected 5000 triples\n",
      "  sample:\n",
      "    [\"'Till I Collapse\", 'composer', 'Eminem']\n",
      "    ['Signs', 'composer', 'Snoop Dogg']\n",
      "    ['La Sylphide', 'composer', 'Jean Schneitzhoeffer']\n",
      "    ['Somewhere I Belong', 'composer', 'Brad Delson']\n",
      "    ['Somewhere I Belong', 'composer', 'Mike Shinoda']\n",
      "    200 triples with sentences in 85.22 seconds!\n",
      "\n",
      "processing \"part of\" (P361) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Étienne', 'part of', 'Labyrinthe']\n",
      "    ['Better Be Good to Me', 'part of', 'Private Dancer']\n",
      "    ['The Show Must Go On', 'part of', 'Innuendo']\n",
      "    [\"I'm Your Man\", 'part of', 'Music from the Edge of Heaven']\n",
      "    ['Suga Mama', 'part of', \"B'Day\"]\n",
      "    200 triples with sentences in 12.37 seconds!\n",
      "\n",
      "processing \"lyrics by\" (P676) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Superstar', 'lyrics by', 'Madonna']\n",
      "    ['Brown Eyed Girl', 'lyrics by', 'Van Morrison']\n",
      "    ['Party in the U.S.A.', 'lyrics by', 'Dr. Luke']\n",
      "    ['The Show Must Go On', 'lyrics by', 'Roger Taylor']\n",
      "    ['The Logical Song', 'lyrics by', 'Rick Davies']\n",
      "    200 triples with sentences in 52.12 seconds!\n",
      "\n",
      "processing \"publication date\" (P577) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Live', 'publication date', '01 October 2012']\n",
      "    ['Live at Knebworth', 'publication date', '24 March 2002']\n",
      "    ['Music in Review', 'publication date', '01 January 2007']\n",
      "    [\"In Concert - Mirage Tour '82\", 'publication date', '01 January 1983']\n",
      "    ['Hilary Duff: Live At Gibson Amphitheatre - August 15th, 2007', 'publication date', '08 June 2010']\n",
      "    200 triples with sentences in 8.77 seconds!\n",
      "\n",
      "processing \"language of work or name\" (P407) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Spem in alium', 'language of work or name', 'Latin']\n",
      "    ['Batte forte', 'language of work or name', 'Italian']\n",
      "    ['I miei pensieri', 'language of work or name', 'Italian']\n",
      "    ['Barcelona', 'language of work or name', 'Spanish']\n",
      "    ['Vivo per lei', 'language of work or name', 'Italian']\n",
      "    13 triples with sentences in 15.64 seconds!\n",
      "\n",
      "processing \"voice type\" (P412) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Domenico Cosselli', 'voice type', 'bass']\n",
      "    ['Natale de Carolis', 'voice type', 'bass']\n",
      "    ['Henri Medus', 'voice type', 'bass']\n",
      "    ['Lkhasaran Linkhovoin', 'voice type', 'bass']\n",
      "    ['Alexander Wesselsky', 'voice type', 'bass']\n",
      "    200 triples with sentences in 4.16 seconds!\n",
      "\n",
      "processing \"instrumentation\" (P870) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Aus Chinesich-deutsche Jahres- und Tageszeiten', 'instrumentation', 'voice']\n",
      "    ['If Ye Love Me', 'instrumentation', 'SATB choir']\n",
      "    ['Ach mein herzliebes Jesulein', 'instrumentation', 'string section']\n",
      "    ['Concerto for Clarinet', 'instrumentation', 'jazz band']\n",
      "    ['Ach mein herzliebes Jesulein', 'instrumentation', 'timpani']\n",
      "    200 triples with sentences in 13.26 seconds!\n",
      "\n",
      "processing \"tracklist\" (P658) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    [\"(What's the Story) Morning Glory?\", 'tracklist', 'Morning Glory']\n",
      "    ['Nevermind', 'tracklist', 'Lithium']\n",
      "    ['Born to Die', 'tracklist', 'Blue Jeans']\n",
      "    ['Born to Die', 'tracklist', 'National Anthem']\n",
      "    ['Born to Die', 'tracklist', 'Off to the Races']\n",
      "    200 triples with sentences in 28.94 seconds!\n",
      "\n",
      "processing \"genre\" (P136) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    [\"I'm a Cult Hero\", 'genre', 'post-punk']\n",
      "    ['Primary', 'genre', 'post-punk']\n",
      "    ['Are You Are Missing Winner', 'genre', 'post-punk']\n",
      "    [\"Boys Don't Cry\", 'genre', 'post-punk']\n",
      "    ['Hitsville UK', 'genre', 'post-punk']\n",
      "    200 triples with sentences in 8.33 seconds!\n",
      "\n",
      "processing \"performer\" (P175) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['East Clintwood / Human Shit', 'performer', 'Antigama']\n",
      "    ['Warfare Noise', 'performer', 'Chakal']\n",
      "    [\"5 O'Clock\", 'performer', 'Lily Allen']\n",
      "    ['Closer', 'performer', 'Nine Inch Nails']\n",
      "    ['Star 69 / Weapon of Choice', 'performer', 'Fatboy Slim']\n",
      "    200 triples with sentences in 9.17 seconds!\n",
      "\n",
      "processing \"producer\" (P162) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['Strawberries Oceans Ships Forest', 'producer', 'Paul McCartney']\n",
      "    ['Everything in Time', 'producer', 'No Doubt']\n",
      "    ['GRRR!', 'producer', 'Andrew Loog Oldham']\n",
      "    ['Beautiful Friction', 'producer', 'The Young Bucks']\n",
      "    ['Black Traffic', 'producer', 'Chris Sheldon']\n",
      "    200 triples with sentences in 9.19 seconds!\n",
      "\n",
      "processing \"nominated for\" (P1411) relation:\n",
      "  419 SPARQL results.\n",
      "  collected 419 triples\n",
      "  sample:\n",
      "    ['Come Around Sundown', 'nominated for', 'Grammy Award for Best Rock Album']\n",
      "    ['Justified', 'nominated for', 'Grammy Award for Album of the Year']\n",
      "    ['Return of Saturn', 'nominated for', 'Grammy Award for Best Rock Album']\n",
      "    ['Black Ice', 'nominated for', 'Grammy Award for Best Rock Album']\n",
      "    ['Judy at Carnegie Hall', 'nominated for', 'Grammy Award for Album of the Year']\n",
      "    24 triples with sentences in 11.53 seconds!\n",
      "\n",
      "processing \"record label\" (P264) relation:\n",
      "  10000 SPARQL results.\n",
      "  collected 10000 triples\n",
      "  sample:\n",
      "    ['A Trick of the Tail', 'record label', 'Virgin Records']\n",
      "    ['Deep Shadows and Brilliant Highlights', 'record label', 'Bertelsmann Music Group']\n",
      "    ['A Trick of the Tail', 'record label', 'Charisma']\n",
      "    ['Girl on Fire', 'record label', 'RCA Records']\n",
      "    ['XX – Two Decades of Love Metal', 'record label', 'Sony BMG']\n",
      "    200 triples with sentences in 10.76 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Update base path to use existing ontology directory\n",
    "base_path = '../../data/wikidata_tekgen/ontologies'\n",
    "\n",
    "# Check if directory exists, if not create it\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "    error_message = \"\"\"\n",
    "ERROR: Ontology files not found!\n",
    "Please copy the original ontology files from the Text2KGBench repository\n",
    "\"\"\".format(base_path)\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Check if directory is empty or missing ontology files\n",
    "ontology_files = [f for f in os.listdir(base_path) if f.endswith('_ontology.json')]\n",
    "if not ontology_files:\n",
    "    error_message = \"\"\"\n",
    "ERROR: Ontology files not found!\n",
    "Please copy the original ontology files from the Text2KGBench repository\n",
    "\"\"\".format(base_path)\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load existing ontologies\n",
    "ontologies = []\n",
    "for filename in os.listdir(base_path):\n",
    "    if filename.endswith('_ontology.json'):\n",
    "        with open(os.path.join(base_path, filename)) as in_file:\n",
    "            ontologies.append(json.load(in_file))\n",
    "show_sample = True\n",
    "show_query = False\n",
    "\n",
    "for onto in ontologies:\n",
    "    print(f\"Ontology: {onto['title']} ({onto['id']})\")\n",
    "    onto_id = onto['id']\n",
    "    train_all, val_all, test_all = [], [],[]\n",
    "    for rel in onto['relations']:\n",
    "        print(f\"\\nprocessing \\\"{rel['label']}\\\" ({rel['pid']}) relation:\")\n",
    "        start_time = time.time()\n",
    "        triples_with_sentences = get_triples_with_sentences(rel['pid'], rel['label'], rel['domain'], rel['range'], 200)\n",
    "        elapsed_time = (time.time()-start_time)\n",
    "        print(f\"    {len(triples_with_sentences)} triples with sentences in {elapsed_time:.2f} seconds!\")\n",
    "        train, val, test = get_splits(triples_with_sentences)\n",
    "        train_all += train\n",
    "        val_all += val\n",
    "        test_all += test\n",
    "    save_triples(onto_id, train_all, val_all, test_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
