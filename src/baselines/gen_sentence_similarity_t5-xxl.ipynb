{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d278f-74e8-44e5-b7a9-2540e287ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360f19e-28fe-4cf9-a8dd-26668635ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file to get the ontology categories\n",
    "config_path = 'config/dbpedia_webnlg_prompt_gen_config.json'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "ontology_list = config['onto_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeba4c6-874d-4ac3-9985-17462f86f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "# Note: T5-XXL is a very large model (11B parameters) and may not run on standard hardware.\n",
    "# Make sure you have the necessary resources before loading this model.\n",
    "model_name = 'sentence-t5-xxl'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a93c6b-d062-40c9-8fb1-e2fafcdb7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "test_data_path = '../../data/dbpedia_webnlg/test/'\n",
    "train_data_path = '../../data/dbpedia_webnlg/train/'\n",
    "output_path = '../../data/dbpedia_webnlg/baselines/test_train_sent_similarity/'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec48edd-ca17-4abe-b7a3-2646fcaddb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of top similar sentences to retrieve\n",
    "top_k = 5\n",
    "\n",
    "# Process each ontology category\n",
    "for ontology in ontology_list:\n",
    "    print(f'Processing ontology: {ontology}')\n",
    "\n",
    "    # Load test data\n",
    "    test_file = os.path.join(test_data_path, f'ont_1_{ontology}_test.jsonl')\n",
    "    test_sentences = []\n",
    "    test_ids = []\n",
    "    with open(test_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            test_sentences.append(data['sent'])\n",
    "            test_ids.append(data['id'])\n",
    "\n",
    "    # Load train data\n",
    "    train_file = os.path.join(train_data_path, f'ont_1_{ontology}_train.jsonl')\n",
    "    train_sentences = []\n",
    "    train_ids = []\n",
    "    with open(train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            train_sentences.append(data['sent'])\n",
    "            train_ids.append(data['id'])\n",
    "\n",
    "    # Compute embeddings for test and train sentences\n",
    "    print('Computing embeddings for test sentences...')\n",
    "    test_embeddings = model.encode(test_sentences, convert_to_tensor=True, show_progress_bar=True)\n",
    "    print('Computing embeddings for train sentences...')\n",
    "    train_embeddings = model.encode(train_sentences, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "    # Compute similarities and find top-k similar sentences for each test sentence\n",
    "    similarity_results = {}\n",
    "    print('Computing similarities and finding top similar sentences...')\n",
    "    for idx, test_embedding in enumerate(tqdm(test_embeddings)):\n",
    "        # Compute cosine similarities\n",
    "        cosine_scores = util.cos_sim(test_embedding, train_embeddings)[0]\n",
    "        # Get the top_k results\n",
    "        top_results = torch.topk(cosine_scores, k=top_k)\n",
    "        similar_train_ids = [train_ids[i] for i in top_results[1]]\n",
    "        # Map test ID to similar train IDs\n",
    "        similarity_results[test_ids[idx]] = similar_train_ids\n",
    "\n",
    "    # Save the results to the output file\n",
    "    output_file = os.path.join(output_path, f'{ontology}_test_train_similarity.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(similarity_results, f, indent=4)\n",
    "\n",
    "    print(f'Results saved to {output_file}\\n')\n",
    "\n",
    "print('Processing completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
